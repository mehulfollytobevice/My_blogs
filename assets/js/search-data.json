{
  
    
        "post0": {
            "title": "Exploring Auto-EDA in Python",
            "content": "EDA (Exploratory Data Analysis) is the process of investigating and analyzing the data to summarize it&#39;s main characteristics and gain a deep understanding of what it represents. Usually, we use data visualization techniques to create plots or visuals which are easier to understand and also help us in spotting patterns within the data. In this blog post, we are going to explore how EDA can be automated so that we can reduce the manual effort and time needed to understand our data. . Currently, there are many libraries in python that allow us to automate our EDA process, but here we are going to use the following three: . Sweetviz | Pandas Profiling | AutoViz | . We will be using the HR Analytics dataset from Kaggle to experiment with Auto-EDA and see how useful it really is. . First things first, let&#39;s install our python libraries and download our data. . Initial Setup . Installing the libraries . First, we are going to install our python libraries. To see the install instructions of the libraries in detail, you can check out their Github repositories: . SweetViz | Pandas Profiling | AutoViz | . We are going to use the &#39;pip&#39; command as usual to install the libraries. . !pip install sweetviz autoviz . Collecting sweetviz Downloading https://files.pythonhosted.org/packages/6c/ab/5ef84ee47e5344e9a23d69e0b52c11aba9d8be658d4e85c3c52d4e7d9130/sweetviz-2.1.0-py3-none-any.whl (15.1MB) |████████████████████████████████| 15.1MB 308kB/s Collecting autoviz Downloading https://files.pythonhosted.org/packages/89/20/8c8c64d5221cfcbc54679f4f048a08292a16dbad178af7c78541aa3af730/autoviz-0.0.81-py3-none-any.whl Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (1.19.5) Collecting tqdm&gt;=4.43.0 Downloading https://files.pythonhosted.org/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75kB) |████████████████████████████████| 81kB 7.9MB/s Requirement already satisfied: scipy&gt;=1.3.2 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (1.4.1) Requirement already satisfied: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (2.11.3) Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,&gt;=0.25.3 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (1.1.5) Requirement already satisfied: matplotlib&gt;=3.1.3 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (3.2.2) Requirement already satisfied: importlib-resources&gt;=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sweetviz) (5.1.2) Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from autoviz) (5.5.0) Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (from autoviz) (0.90) Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from autoviz) (0.11.1) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from autoviz) (0.22.2.post1) Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from autoviz) (1.0.0) Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from autoviz) (0.10.2) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2&gt;=2.11.1-&gt;sweetviz) (2.0.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,&gt;=0.25.3-&gt;sweetviz) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,&gt;=0.25.3-&gt;sweetviz) (2.8.1) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.1.3-&gt;sweetviz) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.1.3-&gt;sweetviz) (1.3.1) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.1.3-&gt;sweetviz) (0.10.0) Requirement already satisfied: zipp&gt;=0.4; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from importlib-resources&gt;=1.2.0-&gt;sweetviz) (3.4.1) Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (2.6.1) Requirement already satisfied: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (0.8.1) Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (0.7.5) Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (4.4.2) Requirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (56.1.0) Requirement already satisfied: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (1.0.18) Requirement already satisfied: traitlets&gt;=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (5.0.5) Requirement already satisfied: pexpect; sys_platform != &#34;win32&#34; in /usr/local/lib/python3.7/dist-packages (from ipython-&gt;autoviz) (4.8.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;autoviz) (1.0.1) Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;autoviz) (5.3.1) Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;autoviz) (7.6.3) Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;autoviz) (5.6.1) Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;autoviz) (4.10.1) Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;autoviz) (5.1.0) Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;autoviz) (5.2.0) Requirement already satisfied: patsy&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels-&gt;autoviz) (0.5.1) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas!=1.0.0,!=1.0.1,!=1.0.2,&gt;=0.25.3-&gt;sweetviz) (1.15.0) Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython-&gt;autoviz) (0.2.5) Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets&gt;=4.2-&gt;ipython-&gt;autoviz) (0.2.0) Requirement already satisfied: ptyprocess&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != &#34;win32&#34;-&gt;ipython-&gt;autoviz) (0.7.0) Requirement already satisfied: tornado&gt;=4 in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;autoviz) (5.1.1) Requirement already satisfied: jupyter-core&gt;=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;autoviz) (4.7.1) Requirement already satisfied: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;autoviz) (0.9.5) Requirement already satisfied: jupyter-client&gt;=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;autoviz) (5.3.5) Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;autoviz) (5.1.3) Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;autoviz) (1.5.0) Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;autoviz) (3.5.1) Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0; python_version &gt;= &#34;3.6&#34; in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;autoviz) (1.0.0) Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;autoviz) (3.3.0) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;autoviz) (0.8.4) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;autoviz) (1.4.3) Requirement already satisfied: entrypoints&gt;=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;autoviz) (0.3) Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;autoviz) (0.7.1) Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;autoviz) (0.4.4) Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole-&gt;jupyter-&gt;autoviz) (1.9.0) Requirement already satisfied: pyzmq&gt;=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole-&gt;jupyter-&gt;autoviz) (22.0.3) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat-&gt;notebook-&gt;jupyter-&gt;autoviz) (2.6.0) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;jupyter-&gt;autoviz) (20.9) Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;jupyter-&gt;autoviz) (0.5.1) Installing collected packages: tqdm, sweetviz, autoviz Found existing installation: tqdm 4.41.1 Uninstalling tqdm-4.41.1: Successfully uninstalled tqdm-4.41.1 Successfully installed autoviz-0.0.81 sweetviz-2.1.0 tqdm-4.60.0 . . Pandas Profiling provides some specific instructions for installation so let&#39;s follow those. . import sys !{sys.executable} -m pip install -U pandas-profiling[notebook] !jupyter nbextension enable --py widgetsnbextension . Collecting pandas-profiling[notebook] Downloading https://files.pythonhosted.org/packages/3b/a3/34519d16e5ebe69bad30c5526deea2c3912634ced7f9b5e6e0bb9dbbd567/pandas_profiling-3.0.0-py2.py3-none-any.whl (248kB) |████████████████████████████████| 256kB 6.8MB/s Collecting PyYAML&gt;=5.0.0 Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB) |████████████████████████████████| 645kB 9.4MB/s Collecting requests&gt;=2.24.0 Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB) |████████████████████████████████| 61kB 6.1MB/s Requirement already satisfied, skipping upgrade: seaborn&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (0.11.1) Requirement already satisfied, skipping upgrade: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (1.1.5) Collecting htmlmin&gt;=0.1.12 Downloading https://files.pythonhosted.org/packages/b3/e7/fcd59e12169de19f0131ff2812077f964c6b960e7c09804d30a7bf2ab461/htmlmin-0.1.12.tar.gz Requirement already satisfied, skipping upgrade: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (2.11.3) Requirement already satisfied, skipping upgrade: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (1.19.5) Requirement already satisfied, skipping upgrade: scipy&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (1.4.1) Requirement already satisfied, skipping upgrade: missingno&gt;=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (0.4.2) Requirement already satisfied, skipping upgrade: tqdm&gt;=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (4.60.0) Collecting phik&gt;=0.11.1 Downloading https://files.pythonhosted.org/packages/b7/ce/193e8ddf62d4be643b9b4b20e8e9c63b2f6a20f92778c0410c629f89bdaa/phik-0.11.2.tar.gz (1.1MB) |████████████████████████████████| 1.1MB 16.3MB/s Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (1.0.1) Collecting tangled-up-in-unicode==0.1.0 Downloading https://files.pythonhosted.org/packages/93/3e/cb354fb2097fcf2fd5b5a342b10ae2a6e9363ba435b64e3e00c414064bc7/tangled_up_in_unicode-0.1.0-py3-none-any.whl (3.1MB) |████████████████████████████████| 3.1MB 29.5MB/s Collecting pydantic&gt;=1.8.1 Downloading https://files.pythonhosted.org/packages/9f/f2/2d5425efe57f6c4e06cbe5e587c1fd16929dcf0eb90bd4d3d1e1c97d1151/pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB) |████████████████████████████████| 10.1MB 32.7MB/s Collecting visions[type_image_path]==0.7.1 Downloading https://files.pythonhosted.org/packages/80/96/01e4ba22cef96ae5035dbcf0451c2f4f859f8f17393b98406b23f0034279/visions-0.7.1-py3-none-any.whl (102kB) |████████████████████████████████| 112kB 40.0MB/s Requirement already satisfied, skipping upgrade: matplotlib&gt;=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (3.2.2) Requirement already satisfied, skipping upgrade: ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34; in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (7.6.3) Requirement already satisfied, skipping upgrade: jupyter-core&gt;=4.6.3; extra == &#34;notebook&#34; in /usr/local/lib/python3.7/dist-packages (from pandas-profiling[notebook]) (4.7.1) Collecting jupyter-client&gt;=6.0.0; extra == &#34;notebook&#34; Downloading https://files.pythonhosted.org/packages/77/e8/c3cf72a32a697256608d5fa96360c431adec6e1c6709ba7f13f99ff5ee04/jupyter_client-6.1.12-py3-none-any.whl (112kB) |████████████████████████████████| 122kB 38.3MB/s Requirement already satisfied, skipping upgrade: chardet&lt;5,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling[notebook]) (3.0.4) Requirement already satisfied, skipping upgrade: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling[notebook]) (2.10) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling[notebook]) (2020.12.5) Requirement already satisfied, skipping upgrade: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling[notebook]) (1.24.3) Requirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3-&gt;pandas-profiling[notebook]) (2018.9) Requirement already satisfied, skipping upgrade: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3-&gt;pandas-profiling[notebook]) (2.8.1) Requirement already satisfied, skipping upgrade: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2&gt;=2.11.1-&gt;pandas-profiling[notebook]) (2.0.0) Requirement already satisfied, skipping upgrade: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic&gt;=1.8.1-&gt;pandas-profiling[notebook]) (3.7.4.3) Requirement already satisfied, skipping upgrade: networkx&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling[notebook]) (2.5.1) Requirement already satisfied, skipping upgrade: bottleneck in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling[notebook]) (1.3.2) Collecting multimethod==1.4 Downloading https://files.pythonhosted.org/packages/7a/d0/ce5ad0392aa12645b7ad91a5983d6b625b704b021d9cd48c587630c1a9ac/multimethod-1.4-py2.py3-none-any.whl Requirement already satisfied, skipping upgrade: attrs&gt;=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling[notebook]) (21.2.0) Requirement already satisfied, skipping upgrade: Pillow; extra == &#34;type_image_path&#34; in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.1-&gt;pandas-profiling[notebook]) (7.1.2) Collecting imagehash; extra == &#34;type_image_path&#34; Downloading https://files.pythonhosted.org/packages/8e/18/9dbb772b5ef73a3069c66bb5bf29b9fb4dd57af0d5790c781c3f559bcca6/ImageHash-4.2.0-py2.py3-none-any.whl (295kB) |████████████████████████████████| 296kB 34.7MB/s Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling[notebook]) (2.4.7) Requirement already satisfied, skipping upgrade: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling[notebook]) (1.3.1) Requirement already satisfied, skipping upgrade: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas-profiling[notebook]) (0.10.0) Requirement already satisfied, skipping upgrade: jupyterlab-widgets&gt;=1.0.0; python_version &gt;= &#34;3.6&#34; in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (1.0.0) Requirement already satisfied, skipping upgrade: nbformat&gt;=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (5.1.3) Requirement already satisfied, skipping upgrade: traitlets&gt;=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (5.0.5) Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (3.5.1) Requirement already satisfied, skipping upgrade: ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34; in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (5.5.0) Requirement already satisfied, skipping upgrade: ipykernel&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (4.10.1) Requirement already satisfied, skipping upgrade: pyzmq&gt;=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client&gt;=6.0.0; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (22.0.3) Requirement already satisfied, skipping upgrade: tornado&gt;=4.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client&gt;=6.0.0; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (5.1.1) Requirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3-&gt;pandas-profiling[notebook]) (1.15.0) Requirement already satisfied, skipping upgrade: decorator&lt;5,&gt;=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx&gt;=2.4-&gt;visions[type_image_path]==0.7.1-&gt;pandas-profiling[notebook]) (4.4.2) Requirement already satisfied, skipping upgrade: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash; extra == &#34;type_image_path&#34;-&gt;visions[type_image_path]==0.7.1-&gt;pandas-profiling[notebook]) (1.1.1) Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (0.2.0) Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (2.6.0) Requirement already satisfied, skipping upgrade: notebook&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (5.3.1) Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (0.7.5) Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (2.6.1) Requirement already satisfied, skipping upgrade: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (0.8.1) Requirement already satisfied, skipping upgrade: pexpect; sys_platform != &#34;win32&#34; in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (4.8.0) Requirement already satisfied, skipping upgrade: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (1.0.18) Requirement already satisfied, skipping upgrade: setuptools&gt;=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (56.1.0) Requirement already satisfied, skipping upgrade: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (1.5.0) Requirement already satisfied, skipping upgrade: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (5.6.1) Requirement already satisfied, skipping upgrade: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (0.9.5) Requirement already satisfied, skipping upgrade: ptyprocess&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != &#34;win32&#34;-&gt;ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (0.7.0) Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0; python_version &gt;= &#34;3.3&#34;-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (0.2.5) Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (3.3.0) Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (0.7.1) Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (0.4.4) Requirement already satisfied, skipping upgrade: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (0.8.4) Requirement already satisfied, skipping upgrade: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (1.4.3) Requirement already satisfied, skipping upgrade: entrypoints&gt;=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (0.3) Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (20.9) Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets&gt;=7.5.1; extra == &#34;notebook&#34;-&gt;pandas-profiling[notebook]) (0.5.1) Building wheels for collected packages: htmlmin, phik Building wheel for htmlmin (setup.py) ... done Created wheel for htmlmin: filename=htmlmin-0.1.12-cp37-none-any.whl size=27085 sha256=2889c06fcf32178bcfcd894aa597f8a6f0651e0e53d1d8a3051fb1deba3f03b4 Stored in directory: /root/.cache/pip/wheels/43/07/ac/7c5a9d708d65247ac1f94066cf1db075540b85716c30255459 Building wheel for phik (setup.py) ... done Created wheel for phik: filename=phik-0.11.2-cp37-none-any.whl size=1107413 sha256=a2c7b4e43b5c0b9fe7c51601923c17c3f39f88ec55bb08e7490cf4a73bd8fdd5 Stored in directory: /root/.cache/pip/wheels/c0/a3/b0/f27b1cfe32ea131a3715169132ff6d85653789e80e966c3bf6 Successfully built htmlmin phik ERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you&#39;ll have requests 2.25.1 which is incompatible. ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you&#39;ll have folium 0.8.3 which is incompatible. ERROR: phik 0.11.2 has requirement scipy&gt;=1.5.2, but you&#39;ll have scipy 1.4.1 which is incompatible. Installing collected packages: PyYAML, requests, htmlmin, phik, tangled-up-in-unicode, pydantic, multimethod, imagehash, visions, jupyter-client, pandas-profiling Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Found existing installation: requests 2.23.0 Uninstalling requests-2.23.0: Successfully uninstalled requests-2.23.0 Found existing installation: jupyter-client 5.3.5 Uninstalling jupyter-client-5.3.5: Successfully uninstalled jupyter-client-5.3.5 Found existing installation: pandas-profiling 1.4.1 Uninstalling pandas-profiling-1.4.1: Successfully uninstalled pandas-profiling-1.4.1 Successfully installed PyYAML-5.4.1 htmlmin-0.1.12 imagehash-4.2.0 jupyter-client-6.1.12 multimethod-1.4 pandas-profiling-3.0.0 phik-0.11.2 pydantic-1.8.2 requests-2.25.1 tangled-up-in-unicode-0.1.0 visions-0.7.1 Enabling notebook extension jupyter-js-widgets/extension... - Validating: OK . . Downloading dataset from Kaggle . This blog post is built using Google Colab. It is a notebook server provided by Google for free. You can also use other services to run the code below but you will have to figure out how to get the dataset. The dataset that we use here is present on Kaggle and you can directly download it from here. . In this notebook, we are going to download the dataset from Kaggle into Google Colab and store it in a directory in our Google Drive. Storing your data in the Drive saves you from the trouble of downloading the dataset every time you start a new session in Colab. . For further guidance read this wonderful article by Mrinali Gupta: How to fetch Kaggle Datasets into Google Colab . So let&#39;s get to it! . First, we need to mount our google drive so that we can access all the folders in the drive. . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . Mounted at /content/gdrive . Then we will using the following code to provide a config path for the Kaggle Json API . import os os.environ[&#39;KAGGLE_CONFIG_DIR&#39;] = &quot;/content/gdrive/My Drive/kaggle/Insurance&quot; . We will change our current directory to where we want the dataset to be downloaded . %cd /content/gdrive/My Drive/kaggle/Insurance . /content/gdrive/My Drive/kaggle/Insurance . Now we can download the dataset from kaggle . !kaggle datasets download -d giripujar/hr-analytics . Downloading hr-analytics.zip to /content/gdrive/My Drive/kaggle/Insurance 0% 0.00/111k [00:00&lt;?, ?B/s] 100% 111k/111k [00:00&lt;00:00, 15.9MB/s] . Let&#39;s unzip the files . !unzip *.zip &amp;&amp; rm *.zip . Archive: hr-analytics.zip inflating: HR_comma_sep.csv . What files are present in the current directory? . !ls . AutoViz_Plots HR_comma_sep.csv kaggle.json . Our directory has a file named &#39;HR_comma_sep.csv&#39;. That is our dataset. . Auto-EDA . What does our data look like? . import pandas as pd df=pd.read_csv(&#39;HR_comma_sep.csv&#39;) df.head(5) . satisfaction_level last_evaluation number_project average_montly_hours time_spend_company Work_accident left promotion_last_5years Department salary . 0 0.38 | 0.53 | 2 | 157 | 3 | 0 | 1 | 0 | sales | low | . 1 0.80 | 0.86 | 5 | 262 | 6 | 0 | 1 | 0 | sales | medium | . 2 0.11 | 0.88 | 7 | 272 | 4 | 0 | 1 | 0 | sales | medium | . 3 0.72 | 0.87 | 5 | 223 | 5 | 0 | 1 | 0 | sales | low | . 4 0.37 | 0.52 | 2 | 159 | 3 | 0 | 1 | 0 | sales | low | . From above, we can see that we have a tabular dataset with 10 columns and a target feature (&#39;left&#39;). We have 4 categorical features (&#39;Department&#39;,&#39;salary&#39;,&#39;Work_accident&#39;,&#39;promotion_last_5years&#39;) apart from the target and 5 numerical features. . The goal of EDA is to gain insights which are not apparent just by looking at the data. To do this, we use different kinds of visualizations and statistical techniques which transform our data into information that we can interpret and analyze. With these insights, we can decide what to do with our data, how to process it, how to model it and how to present it in an understanble form. . In this practical example, we are going to use our Auto-EDA tools and note down the insights gained about the data using each one. This well help us in examining their utility and limitations. . SweetViz . SweetViz allows us to create a report which contains a detailed analysis of the data in each column along with information about the associations between the features. Let&#39;s use SweetViz to create a report and see what insights we can gather from it. . import sweetviz as sv my_report = sv.analyze(df,target_feat=&#39;left&#39;) my_report.show_notebook(w=&#39;100%&#39;) . Insights: . There are no missing values in any of the columns. This is an important insight, since it tells us that we do not need to perform any imputation during our preprocessing steps. It is also important for deciding which columns we want to keep while creating a ML model since missing data can have a signficant impact on the model&#39;s performance. . | For each numerical column, we can see various summary statistics like range, standard deviation, median, average and skewness, etc. This allows us to get a sense the data in these columns. This may be very helpful in cases where we are performing regression where some columns need to be transformed for effective modelling. . | For each column, we can also see the distribution of data represented using histograms with additional information about the target feature embedded in it. Visualizations like these help us in making a tangible narrative and asking questions that might reveal something important. For example, the plot for the average_monthly_hours column shows us that people working the longest hours have the highest percentage of leaving. This might raise questions about the work environent and also the reward system within the company. . | Finally, we can also see the associations/correlations between the features in our dataset. This will help us in removing the redundant features from our dataset when we are creating our ML model. This also helps us in creating a better picture of what is really happening. For instance, number_project is highly correlated with satisfication_level. So, people who are getting more projects to work on are suprisingly less satisfied with their jobs. . | . Pandas profiling . Pandas profiling is an extension of the describe function in pandas. This library allows us to conduct some quick data analysis on a pandas dataframe. It is very useful for an initial investigation into the mysteries of our dataset. . Let&#39;s see what we can do with it. . from pandas_profiling import ProfileReport ireport = ProfileReport(df,explorative=True) ireport.to_notebook_iframe() . Insights: . Pandas profiling gives a lot of information about each column in our dataset. It also tells us how many zero, negative and infinite values are present in each numerical column. This allows us to think about the preprocessing steps with much more clarity. For instance, negative values in a column can be sign of data corruption. It could also be the case that negative values have a specific meaning in our dataset. . | On toggling the details of a column, we can see more information like the common and extreme values in a column. Extreme values can be particularly problematic since they can be outliers which make modelling difficult. We might want to remove these outliers so that our ML model can learn and perform better. . | We can also see the duplicate rows in our data. Duplicate rows can mean a lot of things. Depending on the scenario and how they affect the quality of our analysis we might decide to remove them or keep them. For instance in our example, the duplicate rows represent the employees who have left the company. Since our dataset is imbalanced, these duplicate rows might be an attempt of oversampling in order to improve the balance and create an effective model. . | . We have seen some of the insights we can gather using Pandas Profiling. Now, let&#39;s look at a more visual approach to EDA. SweetViz and Pandas profiling give us a lot of numbers to think about, but Autoviz gives us only visualizations to understand our data. . AutoViz . In his book “Good Charts,” Scott Berinato exclaims: “ A good visualization can communicate the nature and potential impact of information and ideas more powerfully than any other form of communication.” . AutoViz gives us a lot of different visualizations that can be useful in understanding and communicating the story behind our data. This can be tremendously useful when your audience comprises of people from a non-technical background who don&#39;t understand numbers but like pictures. . Let&#39;s use some Autoviz. . from autoviz.AutoViz_Class import AutoViz_Class AV = AutoViz_Class() sep = &#39;,&#39; dft = AV.AutoViz(filename=&quot;&quot;, sep=sep, depVar=&#39;left&#39;, dfte=df, header=0, verbose=1, lowess=True, chart_format=&#39;jpg&#39;, max_rows_analyzed=150000, max_cols_analyzed=30) . Shape of your Data Set: (14999, 10) ############## C L A S S I F Y I N G V A R I A B L E S #################### Classifying variables in data set... Number of Numeric Columns = 2 Number of Integer-Categorical Columns = 3 Number of String-Categorical Columns = 2 Number of Factor-Categorical Columns = 0 Number of String-Boolean Columns = 0 Number of Numeric-Boolean Columns = 2 Number of Discrete String Columns = 0 Number of NLP String Columns = 0 Number of Date Time Columns = 0 Number of ID Columns = 0 Number of Columns to Delete = 0 9 Predictors classified... This does not include the Target column(s) No variables removed since no ID or low-information variables found in data set ################ Binary_Classification VISUALIZATION Started ##################### Data Set Shape: 14999 rows, 10 cols Data Set columns info: * satisfaction_level: 0 nulls, 92 unique vals, most common: {0.1: 358, 0.11: 335} * last_evaluation: 0 nulls, 65 unique vals, most common: {0.55: 358, 0.5: 353} * number_project: 0 nulls, 6 unique vals, most common: {4: 4365, 3: 4055} * average_montly_hours: 0 nulls, 215 unique vals, most common: {156: 153, 135: 153} * time_spend_company: 0 nulls, 8 unique vals, most common: {3: 6443, 2: 3244} * Work_accident: 0 nulls, 2 unique vals, most common: {0: 12830, 1: 2169} * promotion_last_5years: 0 nulls, 2 unique vals, most common: {0: 14680, 1: 319} * Department: 0 nulls, 10 unique vals, most common: {&#39;sales&#39;: 4140, &#39;technical&#39;: 2720} * salary: 0 nulls, 3 unique vals, most common: {&#39;low&#39;: 7316, &#39;medium&#39;: 6446} * left: 0 nulls, 2 unique vals, most common: {0: 11428, 1: 3571} -- Columns to delete: &#39; []&#39; Boolean variables %s &#34; [&#39;Work_accident&#39;, &#39;promotion_last_5years&#39;]&#34; Categorical variables %s (&#34; [&#39;Department&#39;, &#39;salary&#39;, &#39;number_project&#39;, &#39;average_montly_hours&#39;, &#34; &#34;&#39;time_spend_company&#39;, &#39;Work_accident&#39;, &#39;promotion_last_5years&#39;]&#34;) Continuous variables %s &#34; [&#39;satisfaction_level&#39;, &#39;last_evaluation&#39;]&#34; Discrete string variables %s &#39; []&#39; Date and time variables %s &#39; []&#39; ID variables %s &#39; []&#39; Target variable %s &#39; left&#39; . Total Number of Scatter Plots = 3 . Time to run AutoViz (in seconds) = 22.712 ###################### VISUALIZATION Completed ######################## . Insights: . Scatter Plots are an important part of EDA. They show us how our data is spread in the 2D/3D plane where the dimensions represent the features from our dataset. From above, we can see how satisfaction level for employees who have left the company differ from the employees who are still working in the company. . | Visualizing the distribution of each feature based on the value of the target variable can also help in spotting important differences. For example, the normed histogram for average_monthly_hours for the people who left the company is very different from the people still working in the company. . | Autoviz also gives us other useful plots like bar charts, correlation heatmap and box plots to understand the patterns hidden in our data. In our example, the bar chart of average satisfaction_level by number_project shows us that people who have worked on the most number of projects (7) have the lowest satisfaction level. On ther other hand, they have the highest average evaluation scores. . | . Now that we have explored all the three Auto-EDA tools, let&#39;s talk about their utility and limitations. . Conclusion . EDA lets us understand our data and develop a deeper understanding about the real world. In the previous sections, we have looked at different Auto-EDA tools and noted some insights that can they provide us. Although, these tools are tremendously useful, they have their limitations. . Firstly, all the EDA tools we have seen above work very well on tabular datasets, but they aren&#39;t useful for datasets from other domains like computer vision, NLP, etc. . Secondly, they don&#39;t always give us the best information and plots. Autoviz uses some internal heuristics to decide which plots are the most useful, but it is still not clear if these heuristics always work. It is still very hard to answer the question: What information is the most useful for a data scientist? . Finally, it&#39;s very difficult to configure these tools in a way that is best suited for your data. They are not very flexible. Although, we can customize them to some degree, our problems will always have specific requirements. . In conclusion, Auto-EDA is very useful to launch an initial investigation into the data. It gives us the basic information necessary to delve deeper into the data and ask more interesting questions. More importantly, it gives us a way of conducting quick data analysis with much less manual effort. . I hope to see more such tools that will help data scientists tell better stories through their data. . See you on the next adventure!!! .",
            "url": "https://mehulfollytobevice.github.io/My_blogs/eda/data_science/2021/05/24/autoeda.html",
            "relUrl": "/eda/data_science/2021/05/24/autoeda.html",
            "date": " • May 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Small Data And The Power Of Pre-Trained Models",
            "content": "With the availability of better data storage facilities, more powerful computers and low latency networks, the commercial usage of huge datasets is rapidly growing in the field of machine learning. In this blog post, I would like to take a contrary position and advocate for the use of small datasets in making powerful deep learning applications. . Up Until Now . In 2008, Google processed 20 petabytes of data in a day. That was 13 years ago. The amount of data generated each day has been growing ever since. Which begs the question : How do we understand it and what do we do with it? . Big Data Analytics is a branch of computer science focused at analyzing large amounts of data to uncover hidden patterns, correlations and other insights. Some common techniques in Big Data Analytics are data mining, text analytics, predictive analytics, data visualization, AI, machine learning, statistics and natural language processing. There is tremendous utility in analyzing massive amounts of data but there are some inherent difficulties in this domain: . Only the people with enough compute power and data storage capacity can produce competent work. | It requires extra investment for special data processing softwares and hardwares since the traditional ones cannot deal with the huge amount of data. | Training a machine learning model in this scenario takes a lot of time and resources. | . The Big Data approach has led to a belief in the ML community that only large datasets can help you make meaningful ML projects. Although having sufficient good quality data is tremendously important, it is not necessarily true that you need large datasets. The bigger, the better stands to question now. . With advances in deep learning, we now have the capibility of using pre-trained models that need some customization and little amount of data to produce near perfect results. The technique of customizing a pre-trained model is called transfer learning. With transfer learning, the notions of implementing ML in the real world are being challenged and a new optimism about deep learning is rapidly spreading. . Let&#39;s see what transfer learning is and how it works. . Small Data And Transfer Learning . What is transfer learning? . Transfer learning, simply put, is the re-use of pre-trained models to solve a new task. In transfer learning, a machine uses the knowledge gained from a previous task to solve a new task that it was not originally trained for. For example, a image classifier trained for recognizing modes of transport can be used to recognize different car models. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; How does it work? . A pre-trained model has millions of tuned parameters (weights and biases) that are used in performing the calculations which produce the final output predictions for a particular task. Transfer learning makes use of these tuned &#39;numbers&#39; to solve a new task. These tuned &#39;numbers&#39; represent knowledge about the real world that can be directly used in other scenarios where similar patterns occur in the data. . The final layers of a deep learning model determine the form of the output predictions we get. So, all we need to do is to ensure that the final layers of the pre-trained model are configured according to our new task. . We do this by chopping off the final layers of the pre-trained model and replacing them with customised new layers that are suitable for our new task. Then, we train the model using our dataset for a few epochs to fine tune it for the new task. . . Figure 1:This diagram shows how a pre-trained model is converted into a custom model by replacing the final layers. Credits: Satya Mallick, https://learnopencv.com/ . Small data . As opposed to Big Data, Small data is the data that consists of smaller datasets with a few thousand instances of labelled data. This is the kind of data that individuals have the resources to work with. This is the kind of data we find on Kaggle, UCI ML Repo and all over the internet. The Internet is crowded with small datasets that are considered &#39;too small&#39; for making highly accurate deep learning models. But not for long, transfer learning allows us to use this abundance of small datasets to make production ready applications. In the next section, we will see a practical example of how transfer learning helps us to get great results on a small dataset. . A Practical Example . In this section, we are going to use transfer learning to create a model that can identify breeds of cats and dogs. The dataset used in this example has a total of 7393 images which is quite small for a computer vision dataset. The images represent 37 breeds of cats and dogs that are to be classified using our DL model. . We are using fastai to build our DL model and create our predictions. It is an easy to use deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. . Getting the data ready . First things first, let&#39;s download fastai. . ! [ -e /content ] &amp;&amp; pip install -Uqq fastai . |████████████████████████████████| 194kB 13.7MB/s |████████████████████████████████| 12.8MB 24.4MB/s |████████████████████████████████| 61kB 10.3MB/s |████████████████████████████████| 776.8MB 24kB/s ERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you&#39;ll have torch 1.7.1 which is incompatible. . Let&#39;s download the dataset. We are using the PETS dataset which is a standard computer vision dataset for multiclass classification. . from fastai.vision.all import * path=untar_data(URLs.PETS) . Setting the base path of the directory to easy access. . Path.BASE_PATH=path . We have two folders in base directory: images and annotations. Here we will only use the images folder. . path.ls() . (#2) [Path(&#39;images&#39;),Path(&#39;annotations&#39;)] . What do we have in the annotations folder? . (path/&#39;annotations&#39;).ls() . (#7) [Path(&#39;annotations/._trimaps&#39;),Path(&#39;annotations/list.txt&#39;),Path(&#39;annotations/README&#39;),Path(&#39;annotations/xmls&#39;),Path(&#39;annotations/test.txt&#39;),Path(&#39;annotations/trainval.txt&#39;),Path(&#39;annotations/trimaps&#39;)] . What do we have in the images folder? . As you can see, there are 7393 images of dogs and cats. Each file name tells us what breed the pet in the image belongs to. Also, files starting with a capital letter represent the images of cats while files starting with a lower letter represent the images of dogs. . (path/&#39;images&#39;).ls() . (#7393) [Path(&#39;images/pug_6.jpg&#39;),Path(&#39;images/beagle_198.jpg&#39;),Path(&#39;images/miniature_pinscher_163.jpg&#39;),Path(&#39;images/beagle_69.jpg&#39;),Path(&#39;images/beagle_120.jpg&#39;),Path(&#39;images/Persian_52.jpg&#39;),Path(&#39;images/Persian_131.jpg&#39;),Path(&#39;images/Ragdoll_136.jpg&#39;),Path(&#39;images/shiba_inu_4.jpg&#39;),Path(&#39;images/english_setter_80.jpg&#39;)...] . Now we have to preprocess our data and form mini-batches that can be directly fed into the model. Fastai gives us readymade functions that are super useful in getting the data ready. . pets=DataBlock(blocks=(ImageBlock,CategoryBlock), get_items=get_image_files, #getting the images splitter=RandomSplitter(seed=42), #splitting the data into training and validation set get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;),&#39;name&#39;), #labelling the images item_tfms=Resize(460), batch_tfms=aug_transforms(size=224,min_scale=0.75)) #performing data augmentation dls=pets.dataloaders(path/&quot;images&quot;) . We can also look at our data and see what the processed output looks like. . dls.show_batch(nrows=2,ncols=4) . Training . Now that our data is ready, we have to choose a learning rate that is going to give us the best results. To accomplish this, we will use the learning rate finder built-in in fastai. Our learning rate finder tells us which learning rate gives us the fastest decline in the loss function. From below, we can see that in our scenario a learning rate of around 1e-3 would be appropriate. . We are using the Resnet50 pre-trained model to create our customised pet breed identifier. Furthermore, we are going to track the progress of our model by using error rate as the metric. . learner=cnn_learner(dls,resnet50,metrics=error_rate) #we are using resnet50 as our pre-trained model with error rate as our metric lr_min,lr_steep=learner.lr_find() . Let&#39;s fit our pretrained model. . import time start=time.time() learner.fit_one_cycle(3,3e-3) #fine tuning the final layer learner.unfreeze() learner.fit_one_cycle(10,lr_max=slice(1e-5,1e-3)) #fine tuning the rest of the layers fin=time.time() . epoch train_loss valid_loss error_rate time . 0 | 1.433581 | 2.581600 | 0.657645 | 01:04 | . 1 | 1.320106 | 1.112681 | 0.340325 | 01:05 | . 2 | 0.915266 | 0.759420 | 0.232070 | 01:05 | . epoch train_loss valid_loss error_rate time . 0 | 0.746069 | 0.777516 | 0.232747 | 01:06 | . 1 | 0.749254 | 0.734976 | 0.230717 | 01:05 | . 2 | 0.708324 | 0.727369 | 0.228011 | 01:05 | . 3 | 0.664956 | 0.700559 | 0.212449 | 01:05 | . 4 | 0.650105 | 0.680652 | 0.211773 | 01:05 | . 5 | 0.575446 | 0.676955 | 0.211773 | 01:05 | . 6 | 0.565760 | 0.661803 | 0.211096 | 01:04 | . 7 | 0.571792 | 0.641877 | 0.202300 | 01:04 | . 8 | 0.544800 | 0.646405 | 0.197564 | 01:04 | . 9 | 0.558373 | 0.643165 | 0.203654 | 01:04 | . How much time did we train for? . print(f&quot;Model training time: {round((fin-start)/60)} minutes&quot;) . Model training time: 14 minutes . What does the loss look like during training? . learner.recorder.plot_loss() . Model Interpretation . Let&#39;s interpret the results of our model. . interpret=ClassificationInterpretation.from_learner(learner) . We can make a confusion matrix to interpret the results of our model. . interpret.plot_confusion_matrix(figsize=(12,12),dpi=60) . Which pet breeds are the most confusing for our model? . interpret.most_confused()[:5] . [(&#39;Russian_Blue&#39;, &#39;British_Shorthair&#39;, 8), (&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 8), (&#39;english_cocker_spaniel&#39;, &#39;english_setter&#39;, 6), (&#39;Birman&#39;, &#39;Ragdoll&#39;, 5), (&#39;British_Shorthair&#39;, &#39;Russian_Blue&#39;, 5)] . Does it even work? . Building the model and fitting it on our dataset is not sufficient. We need to check if our model really works. Let&#39;s upload a random image and create the predictions. We will be using ipython widgets to create the upload button. . import ipywidgets as wig . Let&#39;s upload the image. . upload= wig.FileUpload() upload . img = PILImage.create(upload.data[-1]) . What does our uploaded image look like? . display(img.to_thumb(256,256)) . Let&#39;s create the predictions. . prediction,pred_idx,probs=learner.predict(img) print(&quot;Our prediction is:&quot;,prediction) . Our prediction is: Russian_Blue . With only 14 minutes of training we are able to make a highly accurate DL model that can classify 37 breeds of cats and dogs. What an amazing result!!! . This is not all. The task demonstrated here is a relatively simple one. With better pre-trained models and a little more training we can accomplish significant feats. The usage of these pre-trained models is not limited to the task they were originally designed for. For example, our Resnet50 model can not only be used for multiclass classification but also for image regression, multilabel classification and even image captioning. Sky is the limit!!! . Model Vendors And The Future . Now that we have seen the power of pre-trained models and transfer learning, let&#39;s think about what the future might look like. . We will still need lots of people working with large datasets. All pre-trained models need to be trained by someone initially. Although training a model like Resnet50 takes a lot of data and time, it can be used by thousands of DL practitioners for making powerful applications afterwards. Moreover, we might see new jobs specifically catering to the growing need of pre-trained models in organizations. We might also see new enterprises that act as Model Vendors. . Model Vendors will be organizations with the resources to develop models like Resnet, BERT, MobileNet, etc. They will sell these pre-trained models to DL practitioners who might use them in production after some customization. This is already happening to some extent with companies like OpenAI and Google who are releasing large pre-trained models (OpenAI&#39;s GPT and Google&#39;s T5). We might see more domain specific models that are not as large but are super useful for domain specialists. . But the most important thing is that we will no longer require large compute and storage resources to make DL applications that will serve the society and solve complex problems. All in all, the future looks optimistic for anybody with a curious mind and a working laptop. . &lt;/div&gt; .",
            "url": "https://mehulfollytobevice.github.io/My_blogs/deep_learning/transfer_learning/2021/05/03/pretrained.html",
            "relUrl": "/deep_learning/transfer_learning/2021/05/03/pretrained.html",
            "date": " • May 3, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "The essentials of data preprocessing for tabular data",
            "content": "If you ask data scientists which part of the ML pipeline takes the most amount of time, they will probably tell you that it&#39;s the data preprocessing stage. Ensuring your data is in the right form before you dump it into your ML/DL model is of paramount importance. If you feed in garbage to your model, you will get garbage as the output. In this blog post we will see some of the essential techniques that you can use to preprocess your data properly. . But first, we need to get some data. . Downloading dataset from Kaggle . This notebook is built using Google Colab. It is a notebook server provided by Google for free. You can also use other services to run the code below but you will have to figure out how to get the dataset. The dataset that we use here is present on Kaggle and you can directly download it from here. . In this notebook, we are going to download the dataset from Kaggle into Google Colab and store it in a directory in our Google Drive. Storing your data in the Drive saves you from the trouble of downloading the dataset every time you start a new session in Colab. . For further guidance read this wonderful article by Mrinali Gupta: How to fetch Kaggle Datasets into Google Colab . So let&#39;s get to it! . First, we need to mount our google drive so that we can access all the folders in the drive. . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . Mounted at /content/gdrive . Then we will using the following code to provide a config path for the Kaggle Json API . import os os.environ[&#39;KAGGLE_CONFIG_DIR&#39;] = &quot;/content/gdrive/My Drive/kaggle/StudentPerformance&quot; . We will change our current directory to where we want the dataset to be downloaded . %cd /content/gdrive/My Drive/kaggle/StudentPerformance . /content/gdrive/My Drive/kaggle/StudentPerformance . Now we can download the dataset from kaggle . !kaggle datasets download -d spscientist/students-performance-in-exams . Downloading students-performance-in-exams.zip to /content/gdrive/My Drive/kaggle/StudentPerformance 0% 0.00/8.70k [00:00&lt;?, ?B/s] 100% 8.70k/8.70k [00:00&lt;00:00, 1.14MB/s] . Let&#39;s unzip the files . !unzip *.zip &amp;&amp; rm *.zip . Archive: students-performance-in-exams.zip inflating: StudentsPerformance.csv . What files are present in the current directory? . !ls . kaggle.json StudentsPerformance.csv . You can see that there is a &quot;StudentsPerformance.csv&quot; file present in the directory. That is our dataset. . Exploring the data . Before we apply any preprocessing steps to the data, we need to know what kind of data the dataset contains. Is it textual data? Is it numerical data? Are there any dates present? What about geographic locations? . There are a lot of questions we can ask about the data in out dataset. So before we move further, we need to get a sense of what hidden knowledge our dataset contains. . In the code cells below you will see some of the most common steps you can apply to gather information about your data. . import pandas as pd df=pd.read_csv(&quot;StudentsPerformance.csv&quot;) . First 5 rows of the dataset Seeing the first and last few rows can be really helpul in creating a mental picture about the data. It also allows you to map out the possible roadblocks you are going to face in acheiving your end goal. . df.head(5) . gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score . 0 female | group B | bachelor&#39;s degree | standard | none | 72 | 72 | 74 | . 1 female | group C | some college | standard | completed | 69 | 90 | 88 | . 2 female | group B | master&#39;s degree | standard | none | 90 | 95 | 93 | . 3 male | group A | associate&#39;s degree | free/reduced | none | 47 | 57 | 44 | . 4 male | group C | some college | standard | none | 76 | 78 | 75 | . Last 5 rows of the dataset . df.tail(5) . gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score . 995 female | group E | master&#39;s degree | standard | completed | 88 | 99 | 95 | . 996 male | group C | high school | free/reduced | none | 62 | 55 | 55 | . 997 female | group C | high school | free/reduced | completed | 59 | 71 | 65 | . 998 female | group D | some college | standard | completed | 68 | 78 | 77 | . 999 female | group D | some college | free/reduced | none | 77 | 86 | 86 | . Information about data type of columns and null values Knowing the data type of each column is crucial in choosing the right preprocessing step for that column as well as understanding what the values in the column represent. . Another crucial piece of information is the number of non-null values. It helps you in deciding which columns need imputation. . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1000 entries, 0 to 999 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 gender 1000 non-null object 1 race/ethnicity 1000 non-null object 2 parental level of education 1000 non-null object 3 lunch 1000 non-null object 4 test preparation course 1000 non-null object 5 math score 1000 non-null int64 6 reading score 1000 non-null int64 7 writing score 1000 non-null int64 dtypes: int64(3), object(5) memory usage: 62.6+ KB . Checking if the dataset has null values In the last cell, we saw how you can see if there are null values in your dataset. Below is another method to confirm if your data has any null/ missing values. As you can see below, this dataset does not have any null values. . for i in list(df.columns): bool_series=pd.isnull(df[i]) print(&quot;Column:{} has {} null values.&quot;.format(i,df[bool_series].shape[0])) . Column:gender has 0 null values. Column:race/ethnicity has 0 null values. Column:parental level of education has 0 null values. Column:lunch has 0 null values. Column:test preparation course has 0 null values. Column:math score has 0 null values. Column:reading score has 0 null values. Column:writing score has 0 null values. . Describing the numerical columns in the dataset If you are applying regression or even classification, knowing the summary statistics might help you in deciding how you want to handle the numerical features. Maybe you have to apply some transformations before you apply regression. Maybe the numerical features can be dropped in case they do not contribute much. . df.describe() . math score reading score writing score . count 1000.00000 | 1000.000000 | 1000.000000 | . mean 66.08900 | 69.169000 | 68.054000 | . std 15.16308 | 14.600192 | 15.195657 | . min 0.00000 | 17.000000 | 10.000000 | . 25% 57.00000 | 59.000000 | 57.750000 | . 50% 66.00000 | 70.000000 | 69.000000 | . 75% 77.00000 | 79.000000 | 79.000000 | . max 100.00000 | 100.000000 | 100.000000 | . How many unique values does each column have You might want to know how many unique values each column has. This is helpful when you have a big dataset and you are thinking of generating more features from the existing features. This is also important when you are dealing with cases where the Curse of Dimensionality becomes relevant. . df.nunique() . gender 2 race/ethnicity 5 parental level of education 6 lunch 2 test preparation course 2 math score 81 reading score 72 writing score 77 dtype: int64 . The essential preprocessing techniques . In this section, we will cover the essential preprocessing techniques you can apply to your data before feeding it into your model. This is by no means an exhaustive list of techniques you can apply, but this covers the most common techinques applied in the ML industry. . The order in which we apply these techniques is very important since each preprocessing step transforms the data such that it may become incompatible for another preprocessing step. . We are going to apply our preprocessing techniques in the following order : . Label Encoding (if needed) | One-hot Encoding | Imputation | Normalization or Standardization | Label Encoding . We saw in the previous section that our data cantains columns which have string values. Although, we can understand what these string values represent, a machine does not. So , to make these values machine-readable we have to find a way to represent them numerically. Label Encoding is one such method of accomplishing this. . In Label Encoding, we assign a unique integer value to each class in the data. This means that the gender column in our dataset will be encoded like so: . Original values | Male | Female | . Label Encoded values | 0 | 1 | &lt;/tr&gt; Let&#39;s see this in action. We are going to label encode the following columns in our dataset: . gender | race/ethnicity | parental level of education | lunch | test preparation course | . from sklearn.preprocessing import LabelEncoder data=df.copy() # creating a copy of our data since we do not want to change the original dataset for i in list(data.columns)[:5]: data[i]=LabelEncoder().fit_transform(data[i]) data.head(5) . gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score . 0 0 | 1 | 1 | 1 | 1 | 72 | 72 | 74 | . 1 0 | 2 | 4 | 1 | 0 | 69 | 90 | 88 | . 2 0 | 1 | 3 | 1 | 1 | 90 | 95 | 93 | . 3 1 | 0 | 0 | 0 | 1 | 47 | 57 | 44 | . 4 1 | 2 | 4 | 1 | 1 | 76 | 78 | 75 | . Although Label Encoding can be useful in many cases, there is a caveat. An algorithm will not be able to differentiate between a numerical variable and a label encoded variable. Due to this limitation, the values in the label encoded columns might be misunderstood. So in our gender column, &#39;1&#39; might be given a higher priority than &#39;0&#39; even when no numerical relation exists between the two classes. . Due to this limitation, we need to find a better way of representing our categorical variables in a numerical form. . One-hot encoding . Due to the shortcomings of Label Encoding, we cannot apply it to transform categorical variables into numerical values. Instead, we use One-Hot Encoding. In One-Hot Encoding, we take the n categories in a column and create n or n-1 columns from them. The new columns contain only binary values (0 or 1). So, if our gender column is one-hot encoded, then we will have two new columns: Male and Female. The values in these columns will be 0 (Male column) and 1 (Female column) if a row earlier had &#39;female&#39; as the gender and vice versa. . Let&#39;s see how to implement this. . df=pd.get_dummies(data=df,drop_first=True) #drop_first can also be False df.head(5) . math score reading score writing score gender_male race/ethnicity_group B race/ethnicity_group C race/ethnicity_group D race/ethnicity_group E parental level of education_bachelor&#39;s degree parental level of education_high school parental level of education_master&#39;s degree parental level of education_some college parental level of education_some high school lunch_standard test preparation course_none . 0 72 | 72 | 74 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | . 1 69 | 90 | 88 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | . 2 90 | 95 | 93 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | . 3 47 | 57 | 44 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 4 76 | 78 | 75 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | . In the above code, the function has a parameter drop_first. What does this mean? In one-hot encoding, we usually drop one of the columns created for each categorical variable to avoid high correlation among the features. But, in many cases dropping columns can have a negative impact on the model performance. So it is always better to experiment and see what works in your case. . One-hot encoding is applied in almost all cases where we have to deal with categorical variables. But when the number of categories in a column gets too big, we cannot use this technique since the resulting dataset might be very large and difficult to handle. In this case, you might want to consider other alternatives like dropping the high-cardinality columns, using label encoding, using dimensionality reduction techniques. . Imputation . Dealing with missing data is another important data preprocessing step. Missing values can have huge impacts on how your model performs. If a significant portion of values in a column are missing then you might consider dropping that column. But,dropping columns might actually result in leaving out essential information.So, we apply imputation. . In imputation, we replace missing data with substituted values. Here, we will dicuss some of the common ways in which imputation is done: . Replace by 0: Sometimes replacing numerical values with 0 can work. Suppose you have an Age column in your dataset. Filling 0 in the places where age is missing might not affect the model&#39;s accuracy. | Replace by mean: You can also take the mean of all the values in the column and use that to fill the missing places. This is the most common imputation approach. But it is very sensitive to outliers. | Replace by most_frequent: You can replace the missing values with the most frequent value. This can work for both categorical and numerical data. | Replace using custom function: If you know how a particular column was generated then you can use that process to fill the missing values. Usually, this approach is not applicable since the data is downloaded from elsewhere. | . In our dataset, we do not have any missing values so we do not need to apply imputation. . For futher guidance: Imputation of missing values . Standardization . Even after converting our data into a machine readable format, our work is not done. In any dataset, you will have parameters that our measured in different units. For example, you might have Time (measured in hours) and Distance (measured in miles).The values of these parameters will have different distributions and different min/max values. You cannot combine these different parameters using a ML model without taking into account their measurement units.So, we use standardization and normalization. . Both of these techniques transform the data in a way such that it either becomes dimensionless (in terms of measurement units) or the parameters end up having similar distributions. . The biggest difference between standardization and normalization is as follows: . Standardization typically rescales the values to have a mean of 0 and a standard deviation of 1 (unit variance). | Normalization typically rescales the values into a range of [0,1]. | . Because of the difference in the way they transform the values we get a different output in each case. . Let&#39;s scale our data. . from sklearn.preprocessing import StandardScaler data=df.copy() data=StandardScaler().fit_transform(data) data[:3] . array([[ 0.39002351, 0.19399858, 0.39149181, -0.96462528, 2.0647416 , -0.68441857, -0.59583014, -0.40347329, 2.73396713, -0.49374193, -0.2503982 , -0.54036068, -0.4669334 , 0.74188112, 0.74674788], [ 0.19207553, 1.42747598, 1.31326868, -0.96462528, -0.4843221 , 1.46109419, -0.59583014, -0.40347329, -0.36576885, -0.49374193, -0.2503982 , 1.85061578, -0.4669334 , 0.74188112, -1.33914006], [ 1.57771141, 1.77010859, 1.64247471, -0.96462528, 2.0647416 , -0.68441857, -0.59583014, -0.40347329, -0.36576885, -0.49374193, 3.99363901, -0.54036068, -0.4669334 , 0.74188112, 0.74674788]]) . Now let&#39;s look at normalization. . Normalization . Normalization typically rescales the values into a range of [0,1]. As you will see below, there is a notable difference between the output of standardization and normalization. Normalization will not transform the values of your categorical/one-hot encoded variables in any way. On the other hand, standardization transforms all the columns in the dataset. . So, let&#39;s normalize our data. . from sklearn.preprocessing import MinMaxScaler data=df.copy() data=MinMaxScaler().fit_transform(data) data[:3] . array([[0.72 , 0.6626506 , 0.71111111, 0. , 1. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 1. , 1. ], [0.69 , 0.87951807, 0.86666667, 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 1. , 0. ], [0.9 , 0.93975904, 0.92222222, 0. , 1. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. , 1. , 1. ]]) . Again, it is very important to experiment with both of these techniques to see which one works for you. . Conclusion . In this blog post we have seen the essentials of data preprocessing for tabular data. My goal with this blog post was to provide a condensed overview of the what data preprocessing looks like with useful code snippets that anyone can use in their projects. Applying proper preprocessing can be extremely helpful in improving the model performance and ensuring that your model is ready for the real world. . If hope you liked this blog post, please share it with other ML enthusiasts. . See you on the next adventure. .",
            "url": "https://mehulfollytobevice.github.io/My_blogs/2021/04/12/preprocess.html",
            "relUrl": "/2021/04/12/preprocess.html",
            "date": " • Apr 12, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "List Comprehensions - The elegant way of creating lists in python",
            "content": "Python is a beautiful programming language.In the recent years, it has become one of the most popular languages and it&#39;s usage in the software industry is rapidly growing.It is also used heavily in the field of machine learning and artificial intelligence.Being a general purpose language , python has a huge community of people from all walks of life who use it to do the most amazing things. I have coded in other languages like C, C++ , java , R but none of them are as elegant as python. I mostly use python for data science and ML but there is nothing you cannot do with python. In this blog post we will explore an elegant and faster way of creating lists in python , list comprehensions. . What are list comprehensions? List comprehensions provide a concise way to create lists .Common applications are to make new lists where each element is the result of some operations applied to each member of another sequence or iterable, or to create a subsequence of those elements that satisfy a certain condition. From python docs . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Why should you use list comprehensions? . 1.They are easy to read and understand As mentioned before , list comprehensions provide a concise way of creating lists . But what does that actually mean?Let&#39;s look at an example. In the example given below , we create a list of even numbers from 1 to 20 using a for loop and then a list comprehension .This is a pretty simple task and does not require a lot of code but it shows us how concise and elegant list comprehensions are. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Implementation using a for loop . l=[] for x in range(1,20): if x%2==0: l.append(x) l . [2, 4, 6, 8, 10, 12, 14, 16, 18] . Implementation using a list comprehension . [x for x in range(1,20) if x%2==0] . [2, 4, 6, 8, 10, 12, 14, 16, 18] . Wow!!! We just needed to write one line of code to create a list that contains all the even numbers from 1 to 20 (20 not included) . Although , the above example performs a very simple task , we now know just how powerful list comprehensions are. Even if we have to do something complex it is always better to use list comprehensions . Since all the logic is packed in one line of code , it is much easier to debug as compared to a for loop with lots of lines and lots of indentation. . 2.They are generally faster than for loops . List comprehensions are gnerally faster than for loops. There are mainly two reasons : . Python is written in C. List comprehensions do a much better job of moving the computation to the C level as compared to the slower computation done by python while we are using a for loop. | While using the for loop , you have to use the append() function in each iteration to add an element to the list you want to create. Calling the append() function each time slows down the execution . | . But why should you believe me? Test it out for yourself ... Python has an in-built module called time that provides us with easy to use functions to track the amount of time our program runs for. . Importing the time library . import time . In this example, we will create a list that contains squares of even numbers and cubes of odd numbers. We will also calculate the amount of time each method takes to complete the computation. . Implementation using for loop . start=time.time() l=[] for i in range(0,20): if i%2==0: l.append(i**2) else: l.append(i**3) print(l) print(&quot;The execution time is:&quot;,time.time()-start) . [0, 1, 4, 27, 16, 125, 36, 343, 64, 729, 100, 1331, 144, 2197, 196, 3375, 256, 4913, 324, 6859] The execution time is: 0.010234832763671875 . Implementation using a list comprehension . start=time.time() l=[x**2 if x % 2 == 0 else x**3 for x in range(0,20)] print(l) print(&quot;The execution time is:&quot;,time.time()-start) . [0, 1, 4, 27, 16, 125, 36, 343, 64, 729, 100, 1331, 144, 2197, 196, 3375, 256, 4913, 324, 6859] The execution time is: 0.00024056434631347656 . Well as you can clearly see, list comprehensions are way faster . But maybe this is just dumb luck . Let&#39;s find a better way to validate the speed of list comprehensions. The best way to test our program is to use different values for our variables and run the program over and over . Earlier , we only calculated the squares and cubes of numbers in the range [1,20] . Now , we are going to use multiple ranges of numbers and see how our execution time changes . Then we wil compare the execution time of the for loop and the list comprehension. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Defining the ranges . ranges=[[0,20],[0,50],[0,100],[0,500],[0,1000]] . Creating a function that uses for loop to complete the given task . def for_loop(r): start=time.time() l=[] for i in range(r[0],r[1]): if i%2==0: l.append(i**2) else: l.append(i**3) print(&quot;The execution time is {} seconds&quot;.format(time.time()-start)) return l . How fast is the for loop at completing the task . for r in ranges: for_loop(r) . The execution time is 2.0742416381835938e-05 seconds The execution time is 3.4332275390625e-05 seconds The execution time is 5.817413330078125e-05 seconds The execution time is 0.0003447532653808594 seconds The execution time is 0.0009100437164306641 seconds . Creating a function that uses list comprehension to complete the given task . def list_comprehension(r): start=time.time() l=[x**2 if x % 2 == 0 else x**3 for x in range(r[0],r[1])] print(&quot;The execution time is {} seconds&quot;.format(time.time()-start)) return l . How fast is the list comprehension at completing the task . for r in ranges: list_comprehension(r) . The execution time is 1.52587890625e-05 seconds The execution time is 3.337860107421875e-05 seconds The execution time is 6.0558319091796875e-05 seconds The execution time is 0.0002884864807128906 seconds The execution time is 0.0005509853363037109 seconds . Comparing the execution times , we observe that list comprehensions are faster even when we increase the range of numbers .Hence proved , list comprehensions are faster than for loops. You can mess around with the above programs by changing the ranges or the function bodies .It is always better to play around with the code and try to break it . . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; For the curious minds out there , here is another method you can use to validate whether list comps are faster than for loops . Defining the ranges . ranges=[[0,20],[0,50],[0,100],[0,500],[0,1000]] . Creating a function that uses for loop to complete the given task . def for_loop(r): start=time.time() l=[] for i in range(r[0],r[1]): if i%2==0: l.append(i**2) else: l.append(i**3) return l, time.time()-start . Creating a function that uses list comprehension to complete the given task . def list_comprehension(r): start=time.time() l=[x**2 if x % 2 == 0 else x**3 for x in range(r[0],r[1])] return l,time.time()-start . Let&#39;s see who is faster . for r in ranges: _,t_for=for_loop(r) _,t_list=list_comprehension(r) if t_for&gt;t_list: print(&quot;List comprehension was faster in case:&quot;,r) else: print(&quot;For loop was faster in case:&quot;,r) . List comprehension was faster in case: [0, 20] List comprehension was faster in case: [0, 50] List comprehension was faster in case: [0, 100] List comprehension was faster in case: [0, 500] List comprehension was faster in case: [0, 1000] . What else ? . It&#39;s not just about readability and speed . List comprehensions are awesome . Let&#39;s see what else we can do with them. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; 1.Nested ifs We can use multiple if conditions in list comps. This increases the flexibility we have in terms of selecting elements from a range or a pre-defined list.It also makes it easier to manage multiple conditions. . Implementation using a for loop . l=[] for y in range(0,100): if y % 2 ==0: if y % 5 == 0: if y % 3 == 0: l.append(y) l . [0, 30, 60, 90] . Implementation using a list comprehension . [y for y in range(100) if y % 2 ==0 if y % 5 == 0 if y % 3==0 ] . [0, 30, 60, 90] . 2.Working with strings Often, we have to create lists from strings using some conditions or functions . Working with strings can become cumbersome if you are using a for loop , or worse , multiple for loops . Here , we will see how list comps can be used to find out the indices of all the vowels in a string. . The string . word=&quot;pneumonoultramicroscopicsilicovolcanoconiosis&quot; . Creating a list of vowels . vowels=[&quot;a&quot;,&quot;e&quot;,&quot;i&quot;,&quot;o&quot;,&quot;u&quot;] . Implementation using for loop . l=[] for idx,x in enumerate(word): if x in vowels: l.append({x:idx}) l . [{&#39;e&#39;: 2}, {&#39;u&#39;: 3}, {&#39;o&#39;: 5}, {&#39;o&#39;: 7}, {&#39;u&#39;: 8}, {&#39;a&#39;: 12}, {&#39;i&#39;: 14}, {&#39;o&#39;: 17}, {&#39;o&#39;: 20}, {&#39;i&#39;: 22}, {&#39;i&#39;: 25}, {&#39;i&#39;: 27}, {&#39;o&#39;: 29}, {&#39;o&#39;: 31}, {&#39;a&#39;: 34}, {&#39;o&#39;: 36}, {&#39;o&#39;: 38}, {&#39;i&#39;: 40}, {&#39;o&#39;: 41}, {&#39;i&#39;: 43}] . Implementation using list comprehension . [{x:idx} for idx,x in enumerate(word) if x in vowels] . [{&#39;e&#39;: 2}, {&#39;u&#39;: 3}, {&#39;o&#39;: 5}, {&#39;o&#39;: 7}, {&#39;u&#39;: 8}, {&#39;a&#39;: 12}, {&#39;i&#39;: 14}, {&#39;o&#39;: 17}, {&#39;o&#39;: 20}, {&#39;i&#39;: 22}, {&#39;i&#39;: 25}, {&#39;i&#39;: 27}, {&#39;o&#39;: 29}, {&#39;o&#39;: 31}, {&#39;a&#39;: 34}, {&#39;o&#39;: 36}, {&#39;o&#39;: 38}, {&#39;i&#39;: 40}, {&#39;o&#39;: 41}, {&#39;i&#39;: 43}] . 3.Multiple for loops The world is a simulation and the source code has multiple for loops. The ability to use multiple for loops to perform complex tasks is very important in the programming world . List comps allow us to use as many for loops as we want inside the brackets. . l=[] for i in [3,5,7,9]: for j in [2,4,6,8]: if i**j &gt; j**i: l.append(i**j) else: l.append(j**i) l . [9, 81, 729, 6561, 32, 1024, 15625, 390625, 128, 16384, 279936, 5764801, 512, 262144, 10077696, 134217728] . [i**j if i**j &gt; j**i else j**i for i in [3,5,7,9] for j in [2,4,6,8]] . [9, 81, 729, 6561, 32, 1024, 15625, 390625, 128, 16384, 279936, 5764801, 512, 262144, 10077696, 134217728] . 4.Nested list comprehensions Often , we need to create a list that is a collection of multiple lists. Using nested list comps , we can again do this with one line of code . Suppose you have to create a program that prints multiplication tables, how would you do it ? Would you use for loops ? How many loops must you create? Also what form should the output be in? Let&#39;s see an easy way you could accomplish this task using list comps. . Implementation using for loop . l=[] for x in range(11): l1=[] for y in range(11): l1.append(x*y) l.append(l1) l . [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20], [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30], [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40], [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50], [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60], [0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70], [0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80], [0, 9, 18, 27, 36, 45, 54, 63, 72, 81, 90], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]] . Implementation using list comprehension . [[x*y for y in range(11)] for x in range(11)] . [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20], [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30], [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40], [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50], [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60], [0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70], [0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80], [0, 9, 18, 27, 36, 45, 54, 63, 72, 81, 90], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]] . Conclusion . List comps are very versatile and you can use them in your projects to make your code faster , readable and modular. But more than that , list comps represent a better way of coding . A way of coding which involves getting rid of excessive clutter , making your programs concise and easier for people to understand.Getting rid of excessive clutter does not only mean making your program smaller, it means formulating your problem in the best possible way. . Albert Einstein used to say: . If you can&#39;t explain it to a six year old, you don&#39;t understand it yourself. . If you can formulate your problem such that you can explain it with one line of code then you have a deep understanding about your problem . This will help you create better systems that not only perform complex tasks but are easier to understand in how they work . . A challenge for you . Okay!!! This has been a fascinating journey . But every journey must end for a new journey to begin . For your new journey to begin , here is a challenge. Given below is a program that can be used to create a list of numbers from the Fibonacci Series , your task is to implement this using list comprehension. It is even possible ? If not , why ? Or maybe there is a way ? See you on the next adventure. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; n=int(input(&quot;Enter the number of digits you want:&quot;)) a=0 #first seed b=1 #second seed l=[1] for _ in range(0,n-1): sum=a+b l.append(sum) a=b b=sum l . Enter the number of digits you want:10 . [1, 1, 2, 3, 5, 8, 13, 21, 34, 55] . &lt;/div&gt; . .",
            "url": "https://mehulfollytobevice.github.io/My_blogs/2021/04/04/list-comprehensions.html",
            "relUrl": "/2021/04/04/list-comprehensions.html",
            "date": " • Apr 4, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Why Don't Engineering Students Think About Ethics",
            "content": "Why don’t engineering students think about ethics. . -Mehul Jain . About a year ago when India was starting to realize that we are dealing with a pandemic and college campuses were still open, I was developing a mobile application to implement contact tracing based on the spread of COVID in other countries. The premise for the app was this: people can voluntarily log their interactions when they go in public spaces using their smartphones and if they contract COVID, that data can be used by the government to trace other people that might have the infection. The mobile application records the location of the interaction along with the details of the people who the individual interacted with. While developing the app I had my doubts regarding its potential misuse and the ethical problems with its implementation, but I was not educated enough to make a detailed assessment about it. I was bothered by the thought of having my interactions logged in some government database, but I was not able to articulate my concern in terms of privacy violation and large-scale surveillance. The project idea was given to me by a professor at my college who had the foresight to see the demand for such an application. Once while discussing how to go about developing the app I voiced my concerns but was told that if the government wants to track people to contain the pandemic, they have a valid reason to do so. I was disappointed after hearing this. Although he had the foresight to think of the application’s potential positive impact, he was not able to think about how it could be misused. This is a part of a larger problem in Indian engineering colleges where ethics isn’t a thing you think about before making something cool . And although I still think it was a good idea, it would have been better if we talked about its potential downsides, consequences and negative impacts. . My experience regarding problematic projects is not limited to the projects I have made. I served as the CSE (Computer Science Engineering) Projects Head of a technical club in my third year and was responsible for looking over all the projects regarding Computer Science in my club. I have seen a ton of students in my club and university making projects that are clearly problematic and need some ethical consideration. It just seems like the universities are not fulfilling the responsibility they have of producing ethical engineers. Clearly there are some inadequacies in the current system that need to be dealt with and solutions need to be provided . . First, let’s explore the reasons why students don’t think about ethical implications of their projects: . Ethics courses are broken: In my college, we have a compulsory course of Ethics and Values that needs to taken in the first year of undergrad. This is the only ethics course students are required to take to complete their credits. Now let’s look at the syllabus of this course. | . Most of the topics on the syllabus are not related to ethics and ethical issues in technology. In the entire course we have only one module that deals with the abuse of technology (Module 7). If you think that students studying technology in an engineering college are taught about how technology is misused in a comprehensive and detailed manner, I’m afraid you are wrong. We are taught about the abuse of technology and its ethical implications for a total of four hours in our first year. It is completely absurd. Now imagine, each batch (in my college) has at least 5000 students graduating from the university each year. This is a huge problem. . Recently, my university has introduced some compulsory courses that deal with information security and general cyber security, but they are not ethics courses. They are technical courses aimed at educating students about how to ensure that your organization is safe from cyber-attacks. . Professors have neither the time and the motivation: In my college, each class has about 60 people and teachers have to attend to multiple batches and multiple courses. It’s only natural that they try to speed up the time they spend with each student so that they can interact with all of the students . The small amount of time that students spend interacting with professors is not enough to discuss the possible ramifications of their work. A lot of times the pacing of the course is such that they are barely able to complete the syllabus let alone discuss about other issues. Even if they have some time, most of the faculty members are full time researchers. They are better off spending their time researching about their field of study rather than ensuring that students know about the ethical implications of the things they are studying. . | Build fast, don’t think about it: Hackathons and coding competitions are an important part of the college experience. Most of the hackathons take place over a period of two days (48 hours and sometimes 72 hours) where students have to implement a solution and pitch it to the judges. Now here is the problem. In these super rapid sprints of developing tech, participants do not have the time to think about the consequences and downsides of their solution. Building tech as fast as possible is a harmful approach that does not foster a sense of responsibility in the people building the tech. Even while pitching the idea , judges do not ask about how the solution can be misused or what are it’s pitfalls. . | I have a personal anecdote to share. One of my friends participated in a hackathon focused on generating solutions to ensure women safety. The solution his team came up with goes something like this: build an app that requires user location to check whether the user is in a ‘safe’ zone or a ‘dangerous’ zone. If the user is in a ‘dangerous’ zone, send them a notification warning them to keep out or inform someone of their whereabouts. At first glance, this solution seems innocuous and even helpful. But there can be some problematic uses of this application. The first obvious concern is regarding the possible leakage and misuse of user location data by companies and malintent individuals. But a more concerning issue is: Who decides which area is ‘dangerous’ and which is ‘safe’ for women. What if companies send warning notifications when the user enters areas where their competitors have set up shop. Asking people to come up with solutions to deep routed problems in our society in 48 hours simply ensures that a new and more pervasive set of problems is created. . Now that we have seen some of the reasons why students don’t think before building, we can provide some helpful suggestions: . Open-source courses are great: There is a great deal of freely available open source content about ethics in tech. I personally recommend Fast AI’s Practical Data Ethics course for anyone who wants to establish a solid understanding about ethical issues in AI. We can learn a lot about how to teach ethics to students from popular open-source courses. An important thing to notice is how practical such courses are. Teaching students about how ethics is applied in real life scenarios must be the top priority of any ethics course. Furthermore, it is important to ensure that ethics courses are based on the current issues in tech and the problems arising now. . | Ramifications section in course based projects: Pushing students to think about the ramifications of their work might be a good thing. Professors can ensure that students include a section about the ramifications of their work when they submit a project report. Although the students might not be able to nail it, it’s important they start somewhere. This may require the teachers to introduce some sort of ethical framework (this may take time) for the students to use but it does ensure that the students think about what they are building. . | Discussions, debates, and talks: We always hear about talks on the benefits of using AI or how blockchain is revolutionizing the banking sector, but we do not hear about the problems that might arise due to the widespread usage of such tech. Colleges have the responsibility of introducing to students the pitfalls and shortcomings of the current technology. Organizing discussions and talks on the problems arising due to tech seems like a reasonably good place to start from in a long journey of producing ethically sound engineers. Debates are an important way of assessing opposing ideas and arguments. They also teach students to think about counter arguments whilst they are formulating their own. . | Promoting research on ethics issues in tech: In 2020, my university had 3061 publications from researchers and students. Clearly a lot of research is being published. But there is a significant lack of research around tech ethics. If the technology institutes in India do not research about the ethical issues in technology then who will? My university has put financial incentives in place to promote students to publish papers and do research, this could be done for promoting research around tech ethics as well. People and groups respond strongly to financial incentives. Maybe if there a cash reward for publishing good research on ethics then researchers and students might focus on it. . | . Technology is a powerful enabler that brings about changes in society that lead to human progress and social good. It is the responsibility of the universities to make sure that the students that go on to become industry leaders have their minds in the right place and their hearts on the right path. .",
            "url": "https://mehulfollytobevice.github.io/My_blogs/2021/03/28/Why-don't-engineering-students-think-about-ethics.html",
            "relUrl": "/2021/03/28/Why-don't-engineering-students-think-about-ethics.html",
            "date": " • Mar 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, I’m Mehul 👋 . I’m a Student Developer and ML enthusiast! . I’m currently working on several ML and data science projects | I love creating amazing things using code | I aspire to become a ML engineer/Data scientist | Fun fact: I love to play drums | . Connect with me: . LinkedIn | Kaggle | Email | . .",
          "url": "https://mehulfollytobevice.github.io/My_blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mehulfollytobevice.github.io/My_blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}