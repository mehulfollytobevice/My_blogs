{
  
    
        "post0": {
            "title": "The essentials of data preprocessing for tabular data",
            "content": "If you ask data scientists which part of the ML pipeline takes the most amount of time, they will probably tell you that it&#39;s the data preprocessing stage. Ensuring your data is in the right form before you dump it into your ML/DL model is of paramount importance. If you feed in garbage to your model, you will get garbage as the output. In this blog post we will see some of the essential techniques that you can use to preprocess your data properly. . But first, we need to get some data. . Downloading dataset from Kaggle . This notebook is built using Google Colab. It is a notebook server provided by Google for free. You can also use other services to run the code below but you will have to figure out how to get the dataset. The dataset that we use here is present on Kaggle and you can directly download it from here. . In this notebook, we are going to download the dataset from Kaggle into Google Colab and store it in a directory in our Google Drive. Storing your data in the Drive saves you from the trouble of downloading the dataset every time you start a new session in Colab. . For further guidance read this wonderful article by Mrinali Gupta: How to fetch Kaggle Datasets into Google Colab . So let&#39;s get to it! . First, we need to mount our google drive so that we can access all the folders in the drive. . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . Mounted at /content/gdrive . Then we will using the following code to provide a config path for the Kaggle Json API . import os os.environ[&#39;KAGGLE_CONFIG_DIR&#39;] = &quot;/content/gdrive/My Drive/kaggle/StudentPerformance&quot; . We will change our current directory to where we want the dataset to be downloaded . %cd /content/gdrive/My Drive/kaggle/StudentPerformance . /content/gdrive/My Drive/kaggle/StudentPerformance . Now we can download the dataset from kaggle . !kaggle datasets download -d spscientist/students-performance-in-exams . Downloading students-performance-in-exams.zip to /content/gdrive/My Drive/kaggle/StudentPerformance 0% 0.00/8.70k [00:00&lt;?, ?B/s] 100% 8.70k/8.70k [00:00&lt;00:00, 1.14MB/s] . Let&#39;s unzip the files . !unzip *.zip &amp;&amp; rm *.zip . Archive: students-performance-in-exams.zip inflating: StudentsPerformance.csv . What files are present in the current directory? . !ls . kaggle.json StudentsPerformance.csv . You can see that there is a &quot;StudentsPerformance.csv&quot; file present in the directory. That is our dataset. . Exploring the data . Before we apply any preprocessing steps to the data, we need to know what kind of data the dataset contains. Is it textual data? Is it numerical data? Are there any dates present? What about geographic locations? . There are a lot of questions we can ask about the data in out dataset. So before we move further, we need to get a sense of what hidden knowledge our dataset contains. . In the code cells below you will see some of the most common steps you can apply to gather information about your data. . import pandas as pd df=pd.read_csv(&quot;StudentsPerformance.csv&quot;) . First 5 rows of the dataset Seeing the first and last few rows can be really helpul in creating a mental picture about the data. It also allows you to map out the possible roadblocks you are going to face in acheiving your end goal. . df.head(5) . gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score . 0 female | group B | bachelor&#39;s degree | standard | none | 72 | 72 | 74 | . 1 female | group C | some college | standard | completed | 69 | 90 | 88 | . 2 female | group B | master&#39;s degree | standard | none | 90 | 95 | 93 | . 3 male | group A | associate&#39;s degree | free/reduced | none | 47 | 57 | 44 | . 4 male | group C | some college | standard | none | 76 | 78 | 75 | . Last 5 rows of the dataset . df.tail(5) . gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score . 995 female | group E | master&#39;s degree | standard | completed | 88 | 99 | 95 | . 996 male | group C | high school | free/reduced | none | 62 | 55 | 55 | . 997 female | group C | high school | free/reduced | completed | 59 | 71 | 65 | . 998 female | group D | some college | standard | completed | 68 | 78 | 77 | . 999 female | group D | some college | free/reduced | none | 77 | 86 | 86 | . Information about data type of columns and null values Knowing the data type of each column is crucial is choosing the right preprocessing step for that column as well as understanding what the values in the column represent. . Another crucial piece of information is the number of non-null values. It helps you in deciding which columns need imputation. . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1000 entries, 0 to 999 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 gender 1000 non-null object 1 race/ethnicity 1000 non-null object 2 parental level of education 1000 non-null object 3 lunch 1000 non-null object 4 test preparation course 1000 non-null object 5 math score 1000 non-null int64 6 reading score 1000 non-null int64 7 writing score 1000 non-null int64 dtypes: int64(3), object(5) memory usage: 62.6+ KB . Checking if the dataset has null values In the last cell, we saw how you can see if there are null values in your dataset. Below is another method to confirm if your data has any null/ missing values. As you can see below, this dataset does not have any null values. . for i in list(df.columns): bool_series=pd.isnull(df[i]) print(&quot;Column:{} has {} null values.&quot;.format(i,df[bool_series].shape[0])) . Column:gender has 0 null values. Column:race/ethnicity has 0 null values. Column:parental level of education has 0 null values. Column:lunch has 0 null values. Column:test preparation course has 0 null values. Column:math score has 0 null values. Column:reading score has 0 null values. Column:writing score has 0 null values. . Describing the numerical columns in the dataset If you are applying regression or even classification, knowing the summary statistics might help you in deciding how you want to handle the numerical features. Maybe you have to apply some transformations before you apply regression. Maybe the numerical features can be dropped in case they do not contribute much. . df.describe() . math score reading score writing score . count 1000.00000 | 1000.000000 | 1000.000000 | . mean 66.08900 | 69.169000 | 68.054000 | . std 15.16308 | 14.600192 | 15.195657 | . min 0.00000 | 17.000000 | 10.000000 | . 25% 57.00000 | 59.000000 | 57.750000 | . 50% 66.00000 | 70.000000 | 69.000000 | . 75% 77.00000 | 79.000000 | 79.000000 | . max 100.00000 | 100.000000 | 100.000000 | . How many unique values does each column have You might want to know how many unique values each column has. This is helpful when you have a big dataset and you are thinking of generating more features from the existing features. This is also important when you are dealing with cases where the Curse of Dimensionality becomes relevant. . df.nunique() . gender 2 race/ethnicity 5 parental level of education 6 lunch 2 test preparation course 2 math score 81 reading score 72 writing score 77 dtype: int64 . The essential preprocessing techniques . In this section, we will cover the essential preprocessing techniques you can apply to your data before feeding it into your model. This is by no means an exhaustive list of techniques you can apply, but this covers the most common techinques applied in the ML industry. . The order in which we apply these techniques is very important since each preprocessing step transforms the data such that it may become incompatible for another preprocessing step. . We are going to apply our preprocessing techniques in the following order : . Label Encoding (if needed) | One-hot Encoding | Imputation | Normalization or Standardization | Label Encoding . We saw in the previous section that our data cantains columns which have string values. Although, we can understand what these string values represent, a machine does not. So , to make these values machine-readable we have to find a way to represent them numerically. Label Encoding is one such method of accomplishing this. . In Label Encoding, we assign a unique integer value to each class in the data. This means that the gender column in our dataset will be encoded like so: . Original values | Male | Female | . Label Encoded values | 0 | 1 | &lt;/tr&gt; Let&#39;s see this in action. We are going to label encode the following columns in our dataset: . gender | race/ethnicity | parental level of education | lunch | test preparation course | . from sklearn.preprocessing import LabelEncoder data=df.copy() # creating a copy of our data since we do not want to change the original dataset for i in list(data.columns)[:5]: data[i]=LabelEncoder().fit_transform(data[i]) data.head(5) . gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score . 0 0 | 1 | 1 | 1 | 1 | 72 | 72 | 74 | . 1 0 | 2 | 4 | 1 | 0 | 69 | 90 | 88 | . 2 0 | 1 | 3 | 1 | 1 | 90 | 95 | 93 | . 3 1 | 0 | 0 | 0 | 1 | 47 | 57 | 44 | . 4 1 | 2 | 4 | 1 | 1 | 76 | 78 | 75 | . Although Label Encoding can be useful in many cases, there is a caveat. An algorithm will not be able to differentiate between a numerical variable and a label encoded variable. Due to this limitation, the values in the label encoded columns might be misunderstood. So in our gender column, &#39;1&#39; might be given a higher priority than &#39;0&#39; even when no numerical relation exists between the two classes. . Due to this limitation, we need to find a better way of representing our categorical variables in a numerical form. . One-hot encoding . Due to the shortcomings of Label Encoding, we cannot apply it to transform categorical variables into numerical values. Instead, we use One-Hot Encoding. In One-Hot Encoding, we take the n categories in a column and create n or n-1 columns from them. The new columns contain only binary values (0 or 1). So, if our gender column is one-hot encoded, then we will have two new columns: Male and Female. The values in these columns will be 0 (Male column) and 1 (Female column) if a row earlier had &#39;female&#39; as the gender and vice versa. . Let&#39;s see how to implement this. . df=pd.get_dummies(data=df,drop_first=True) #drop_first can also be False df.head(5) . math score reading score writing score gender_male race/ethnicity_group B race/ethnicity_group C race/ethnicity_group D race/ethnicity_group E parental level of education_bachelor&#39;s degree parental level of education_high school parental level of education_master&#39;s degree parental level of education_some college parental level of education_some high school lunch_standard test preparation course_none . 0 72 | 72 | 74 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | . 1 69 | 90 | 88 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | . 2 90 | 95 | 93 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | . 3 47 | 57 | 44 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 4 76 | 78 | 75 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | . In the above code, the function has a parameter drop_first. What does this mean? In one-hot encoding, we usually drop one of the columns created for each categorical variable to avoid high correlation among the features. But, in many cases dropping columns can have a negative impact on the model performance. So it is always better to experiment and see what works in your case. . One-hot encoding is applied in almost all cases where we have to deal with categorical variables. But when the number of categories in a column gets too big, we cannot use this technique since the resulting dataset might be very large and difficult to handle. In this case, you might want to consider other alternatives like dropping the high-cardinality columns, using label encoding, using dimensionality reduction techniques. . Imputation . Dealing with missing data is another important data preprocessing step. Missing values can have huge impacts on how your model performs. If a significant portion of values in a column are missing then you might consider dropping that column. But,dropping columns might actually result in leaving out essential information.So, we apply imputation. . In imputation, we replace missing data with substituted values. Here, we will dicuss some of the common ways in which imputation is done: . Replace by 0: Sometimes replacing numerical values with 0 can work. Suppose you have an Age column in your dataset. Filling 0 in the places where age is missing might not affect the model&#39;s accuracy. | Replace by mean: You can also take the mean of all the values in the column and use that to fill the missing places. This is the most common imputation approach. But it is very sensitive to outliers. | Replace by most_frequent: You can replace the missing values with the most frequent value. This can work for both categorical and numerical data. | Replace using custom function: If you know how a particular column was generated then you can use that process to fill the missing values. Usually, this approach is not applicable since the data is downloaded from elsewhere. | . In our dataset, we do not have any missing values so we do not need to apply imputation. . For futher guidance: Imputation of missing values . Standardization . Even after converting our data into a machine readable format, our work is not done. In any dataset, you will have parameters that our measured in different units. For example, you might have Time (measured in hours) and Distance (measured in miles).The values of these parameters will have different distributions and different min/max values. You cannot combine these different parameters using a ML model without taking into account their measurement units.So, we use standardization and normalization. . Both of these techniques transform the data in a way such that it either becomes dimensionless (in terms of measurement units) or the parameters end up having similar distributions. . The biggest difference between standardization and normalization is as follows: . Standardization typically rescales the values to have a mean of 0 and a standard deviation of 1 (unit variance). | Normalization typically rescales the values into a range of [0,1]. | . Because of the difference in the way they transform the values we get a different output in each case. . Let&#39;s scale our data. . from sklearn.preprocessing import StandardScaler data=df.copy() data=StandardScaler().fit_transform(data) data[:3] . array([[ 0.39002351, 0.19399858, 0.39149181, -0.96462528, 2.0647416 , -0.68441857, -0.59583014, -0.40347329, 2.73396713, -0.49374193, -0.2503982 , -0.54036068, -0.4669334 , 0.74188112, 0.74674788], [ 0.19207553, 1.42747598, 1.31326868, -0.96462528, -0.4843221 , 1.46109419, -0.59583014, -0.40347329, -0.36576885, -0.49374193, -0.2503982 , 1.85061578, -0.4669334 , 0.74188112, -1.33914006], [ 1.57771141, 1.77010859, 1.64247471, -0.96462528, 2.0647416 , -0.68441857, -0.59583014, -0.40347329, -0.36576885, -0.49374193, 3.99363901, -0.54036068, -0.4669334 , 0.74188112, 0.74674788]]) . Now let&#39;s look at normalization. . Normalization . Normalization typically rescales the values into a range of [0,1]. As you will see below, there is a notable difference between the output of standardization and normalization. Normalization will not transform the values of your categorical/one-hot encoded variables in any way. On the other hand, standardization transforms all the columns in the dataset. . So, let&#39;s normalize our data. . from sklearn.preprocessing import MinMaxScaler data=df.copy() data=MinMaxScaler().fit_transform(data) data[:3] . array([[0.72 , 0.6626506 , 0.71111111, 0. , 1. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 1. , 1. ], [0.69 , 0.87951807, 0.86666667, 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 1. , 0. ], [0.9 , 0.93975904, 0.92222222, 0. , 1. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. , 1. , 1. ]]) . Again, it is very important to experiment with both of these techniques to see which one works for you. . Conclusion . In this blog post we have seen the essentials of data preprocessing for tabular data. My goal with this blog post was to provide a condensed overview of the what data preprocessing looks like with useful code snippets that anyone can use in their projects. Applying proper preprocessing can be extremely helpful in improving the model performance and ensuring that your model is ready for the real world. . If hope you liked this blog post, please share it with other ML enthusiasts. . See you on the next adventure. .",
            "url": "https://mehulfollytobevice.github.io/My_blogs/2021/04/12/preprocess.html",
            "relUrl": "/2021/04/12/preprocess.html",
            "date": " • Apr 12, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "List Comprehensions - The elegant way of creating lists in python",
            "content": "Python is a beautiful programming language.In the recent years, it has become one of the most popular languages and it&#39;s usage in the software industry is rapidly growing.It is also used heavily in the field of machine learning and artificial intelligence.Being a general purpose language , python has a huge community of people from all walks of life who use it to do the most amazing things. I have coded in other languages like C, C++ , java , R but none of them are as elegant as python. I mostly use python for data science and ML but there is nothing you cannot do with python. In this blog post we will explore an elegant and faster way of creating lists in python , list comprehensions. . What are list comprehensions? List comprehensions provide a concise way to create lists .Common applications are to make new lists where each element is the result of some operations applied to each member of another sequence or iterable, or to create a subsequence of those elements that satisfy a certain condition. From python docs . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Why should you use list comprehensions? . 1.They are easy to read and understand As mentioned before , list comprehensions provide a concise way of creating lists . But what does that actually mean?Let&#39;s look at an example. In the example given below , we create a list of even numbers from 1 to 20 using a for loop and then a list comprehension .This is a pretty simple task and does not require a lot of code but it shows us how concise and elegant list comprehensions are. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Implementation using a for loop . l=[] for x in range(1,20): if x%2==0: l.append(x) l . [2, 4, 6, 8, 10, 12, 14, 16, 18] . Implementation using a list comprehension . [x for x in range(1,20) if x%2==0] . [2, 4, 6, 8, 10, 12, 14, 16, 18] . Wow!!! We just needed to write one line of code to create a list that contains all the even numbers from 1 to 20 (20 not included) . Although , the above example performs a very simple task , we now know just how powerful list comprehensions are. Even if we have to do something complex it is always better to use list comprehensions . Since all the logic is packed in one line of code , it is much easier to debug as compared to a for loop with lots of lines and lots of indentation. . 2.They are generally faster than for loops . List comprehensions are gnerally faster than for loops. There are mainly two reasons : . Python is written in C. List comprehensions do a much better job of moving the computation to the C level as compared to the slower computation done by python while we are using a for loop. | While using the for loop , you have to use the append() function in each iteration to add an element to the list you want to create. Calling the append() function each time slows down the execution . | . But why should you believe me? Test it out for yourself ... Python has an in-built module called time that provides us with easy to use functions to track the amount of time our program runs for. . Importing the time library . import time . In this example, we will create a list that contains squares of even numbers and cubes of odd numbers. We will also calculate the amount of time each method takes to complete the computation. . Implementation using for loop . start=time.time() l=[] for i in range(0,20): if i%2==0: l.append(i**2) else: l.append(i**3) print(l) print(&quot;The execution time is:&quot;,time.time()-start) . [0, 1, 4, 27, 16, 125, 36, 343, 64, 729, 100, 1331, 144, 2197, 196, 3375, 256, 4913, 324, 6859] The execution time is: 0.010234832763671875 . Implementation using a list comprehension . start=time.time() l=[x**2 if x % 2 == 0 else x**3 for x in range(0,20)] print(l) print(&quot;The execution time is:&quot;,time.time()-start) . [0, 1, 4, 27, 16, 125, 36, 343, 64, 729, 100, 1331, 144, 2197, 196, 3375, 256, 4913, 324, 6859] The execution time is: 0.00024056434631347656 . Well as you can clearly see, list comprehensions are way faster . But maybe this is just dumb luck . Let&#39;s find a better way to validate the speed of list comprehensions. The best way to test our program is to use different values for our variables and run the program over and over . Earlier , we only calculated the squares and cubes of numbers in the range [1,20] . Now , we are going to use multiple ranges of numbers and see how our execution time changes . Then we wil compare the execution time of the for loop and the list comprehension. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Defining the ranges . ranges=[[0,20],[0,50],[0,100],[0,500],[0,1000]] . Creating a function that uses for loop to complete the given task . def for_loop(r): start=time.time() l=[] for i in range(r[0],r[1]): if i%2==0: l.append(i**2) else: l.append(i**3) print(&quot;The execution time is {} seconds&quot;.format(time.time()-start)) return l . How fast is the for loop at completing the task . for r in ranges: for_loop(r) . The execution time is 2.0742416381835938e-05 seconds The execution time is 3.4332275390625e-05 seconds The execution time is 5.817413330078125e-05 seconds The execution time is 0.0003447532653808594 seconds The execution time is 0.0009100437164306641 seconds . Creating a function that uses list comprehension to complete the given task . def list_comprehension(r): start=time.time() l=[x**2 if x % 2 == 0 else x**3 for x in range(r[0],r[1])] print(&quot;The execution time is {} seconds&quot;.format(time.time()-start)) return l . How fast is the list comprehension at completing the task . for r in ranges: list_comprehension(r) . The execution time is 1.52587890625e-05 seconds The execution time is 3.337860107421875e-05 seconds The execution time is 6.0558319091796875e-05 seconds The execution time is 0.0002884864807128906 seconds The execution time is 0.0005509853363037109 seconds . Comparing the execution times , we observe that list comprehensions are faster even when we increase the range of numbers .Hence proved , list comprehensions are faster than for loops. You can mess around with the above programs by changing the ranges or the function bodies .It is always better to play around with the code and try to break it . . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; For the curious minds out there , here is another method you can use to validate whether list comps are faster than for loops . Defining the ranges . ranges=[[0,20],[0,50],[0,100],[0,500],[0,1000]] . Creating a function that uses for loop to complete the given task . def for_loop(r): start=time.time() l=[] for i in range(r[0],r[1]): if i%2==0: l.append(i**2) else: l.append(i**3) return l, time.time()-start . Creating a function that uses list comprehension to complete the given task . def list_comprehension(r): start=time.time() l=[x**2 if x % 2 == 0 else x**3 for x in range(r[0],r[1])] return l,time.time()-start . Let&#39;s see who is faster . for r in ranges: _,t_for=for_loop(r) _,t_list=list_comprehension(r) if t_for&gt;t_list: print(&quot;List comprehension was faster in case:&quot;,r) else: print(&quot;For loop was faster in case:&quot;,r) . List comprehension was faster in case: [0, 20] List comprehension was faster in case: [0, 50] List comprehension was faster in case: [0, 100] List comprehension was faster in case: [0, 500] List comprehension was faster in case: [0, 1000] . What else ? . It&#39;s not just about readability and speed . List comprehensions are awesome . Let&#39;s see what else we can do with them. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; 1.Nested ifs We can use multiple if conditions in list comps. This increases the flexibility we have in terms of selecting elements from a range or a pre-defined list.It also makes it easier to manage multiple conditions. . Implementation using a for loop . l=[] for y in range(0,100): if y % 2 ==0: if y % 5 == 0: if y % 3 == 0: l.append(y) l . [0, 30, 60, 90] . Implementation using a list comprehension . [y for y in range(100) if y % 2 ==0 if y % 5 == 0 if y % 3==0 ] . [0, 30, 60, 90] . 2.Working with strings Often, we have to create lists from strings using some conditions or functions . Working with strings can become cumbersome if you are using a for loop , or worse , multiple for loops . Here , we will see how list comps can be used to find out the indices of all the vowels in a string. . The string . word=&quot;pneumonoultramicroscopicsilicovolcanoconiosis&quot; . Creating a list of vowels . vowels=[&quot;a&quot;,&quot;e&quot;,&quot;i&quot;,&quot;o&quot;,&quot;u&quot;] . Implementation using for loop . l=[] for idx,x in enumerate(word): if x in vowels: l.append({x:idx}) l . [{&#39;e&#39;: 2}, {&#39;u&#39;: 3}, {&#39;o&#39;: 5}, {&#39;o&#39;: 7}, {&#39;u&#39;: 8}, {&#39;a&#39;: 12}, {&#39;i&#39;: 14}, {&#39;o&#39;: 17}, {&#39;o&#39;: 20}, {&#39;i&#39;: 22}, {&#39;i&#39;: 25}, {&#39;i&#39;: 27}, {&#39;o&#39;: 29}, {&#39;o&#39;: 31}, {&#39;a&#39;: 34}, {&#39;o&#39;: 36}, {&#39;o&#39;: 38}, {&#39;i&#39;: 40}, {&#39;o&#39;: 41}, {&#39;i&#39;: 43}] . Implementation using list comprehension . [{x:idx} for idx,x in enumerate(word) if x in vowels] . [{&#39;e&#39;: 2}, {&#39;u&#39;: 3}, {&#39;o&#39;: 5}, {&#39;o&#39;: 7}, {&#39;u&#39;: 8}, {&#39;a&#39;: 12}, {&#39;i&#39;: 14}, {&#39;o&#39;: 17}, {&#39;o&#39;: 20}, {&#39;i&#39;: 22}, {&#39;i&#39;: 25}, {&#39;i&#39;: 27}, {&#39;o&#39;: 29}, {&#39;o&#39;: 31}, {&#39;a&#39;: 34}, {&#39;o&#39;: 36}, {&#39;o&#39;: 38}, {&#39;i&#39;: 40}, {&#39;o&#39;: 41}, {&#39;i&#39;: 43}] . 3.Multiple for loops The world is a simulation and the source code has multiple for loops. The ability to use multiple for loops to perform complex tasks is very important in the programming world . List comps allow us to use as many for loops as we want inside the brackets. . l=[] for i in [3,5,7,9]: for j in [2,4,6,8]: if i**j &gt; j**i: l.append(i**j) else: l.append(j**i) l . [9, 81, 729, 6561, 32, 1024, 15625, 390625, 128, 16384, 279936, 5764801, 512, 262144, 10077696, 134217728] . [i**j if i**j &gt; j**i else j**i for i in [3,5,7,9] for j in [2,4,6,8]] . [9, 81, 729, 6561, 32, 1024, 15625, 390625, 128, 16384, 279936, 5764801, 512, 262144, 10077696, 134217728] . 4.Nested list comprehensions Often , we need to create a list that is a collection of multiple lists. Using nested list comps , we can again do this with one line of code . Suppose you have to create a program that prints multiplication tables, how would you do it ? Would you use for loops ? How many loops must you create? Also what form should the output be in? Let&#39;s see an easy way you could accomplish this task using list comps. . Implementation using for loop . l=[] for x in range(11): l1=[] for y in range(11): l1.append(x*y) l.append(l1) l . [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20], [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30], [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40], [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50], [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60], [0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70], [0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80], [0, 9, 18, 27, 36, 45, 54, 63, 72, 81, 90], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]] . Implementation using list comprehension . [[x*y for y in range(11)] for x in range(11)] . [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20], [0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 30], [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40], [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50], [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60], [0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 70], [0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80], [0, 9, 18, 27, 36, 45, 54, 63, 72, 81, 90], [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]] . Conclusion . List comps are very versatile and you can use them in your projects to make your code faster , readable and modular. But more than that , list comps represent a better way of coding . A way of coding which involves getting rid of excessive clutter , making your programs concise and easier for people to understand.Getting rid of excessive clutter does not only mean making your program smaller, it means formulating your problem in the best possible way. . Albert Einstein used to say: . If you can&#39;t explain it to a six year old, you don&#39;t understand it yourself. . If you can formulate your problem such that you can explain it with one line of code then you have a deep understanding about your problem . This will help you create better systems that not only perform complex tasks but are easier to understand in how they work . . A challenge for you . Okay!!! This has been a fascinating journey . But every journey must end for a new journey to begin . For your new journey to begin , here is a challenge. Given below is a program that can be used to create a list of numbers from the Fibonacci Series , your task is to implement this using list comprehension. It is even possible ? If not , why ? Or maybe there is a way ? See you on the next adventure. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; n=int(input(&quot;Enter the number of digits you want:&quot;)) a=0 #first seed b=1 #second seed l=[1] for _ in range(0,n-1): sum=a+b l.append(sum) a=b b=sum l . Enter the number of digits you want:10 . [1, 1, 2, 3, 5, 8, 13, 21, 34, 55] . &lt;/div&gt; . .",
            "url": "https://mehulfollytobevice.github.io/My_blogs/2021/04/04/list-comprehensions.html",
            "relUrl": "/2021/04/04/list-comprehensions.html",
            "date": " • Apr 4, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Why Don't Engineering Students Think About Ethics",
            "content": "Why don’t engineering students think about ethics. . -Mehul Jain . About a year ago when India was starting to realize that we are dealing with a pandemic and college campuses were still open, I was developing a mobile application to implement contact tracing based on the spread of COVID in other countries. The premise for the app was this: people can voluntarily log their interactions when they go in public spaces using their smartphones and if they contract COVID, that data can be used by the government to trace other people that might have the infection. The mobile application records the location of the interaction along with the details of the people who the individual interacted with. While developing the app I had my doubts regarding its potential misuse and the ethical problems with its implementation, but I was not educated enough to make a detailed assessment about it. I was bothered by the thought of having my interactions logged in some government database, but I was not able to articulate my concern in terms of privacy violation and large-scale surveillance. The project idea was given to me by a professor at my college who had the foresight to see the demand for such an application. Once while discussing how to go about developing the app I voiced my concerns but was told that if the government wants to track people to contain the pandemic, they have a valid reason to do so. I was disappointed after hearing this. Although he had the foresight to think of the application’s potential positive impact, he was not able to think about how it could be misused. This is a part of a larger problem in Indian engineering colleges where ethics isn’t a thing you think about before making something cool . And although I still think it was a good idea, it would have been better if we talked about its potential downsides, consequences and negative impacts. . My experience regarding problematic projects is not limited to the projects I have made. I served as the CSE (Computer Science Engineering) Projects Head of a technical club in my third year and was responsible for looking over all the projects regarding Computer Science in my club. I have seen a ton of students in my club and university making projects that are clearly problematic and need some ethical consideration. It just seems like the universities are not fulfilling the responsibility they have of producing ethical engineers. Clearly there are some inadequacies in the current system that need to be dealt with and solutions need to be provided . . First, let’s explore the reasons why students don’t think about ethical implications of their projects: . Ethics courses are broken: In my college, we have a compulsory course of Ethics and Values that needs to taken in the first year of undergrad. This is the only ethics course students are required to take to complete their credits. Now let’s look at the syllabus of this course. | . Most of the topics on the syllabus are not related to ethics and ethical issues in technology. In the entire course we have only one module that deals with the abuse of technology (Module 7). If you think that students studying technology in an engineering college are taught about how technology is misused in a comprehensive and detailed manner, I’m afraid you are wrong. We are taught about the abuse of technology and its ethical implications for a total of four hours in our first year. It is completely absurd. Now imagine, each batch (in my college) has at least 5000 students graduating from the university each year. This is a huge problem. . Recently, my university has introduced some compulsory courses that deal with information security and general cyber security, but they are not ethics courses. They are technical courses aimed at educating students about how to ensure that your organization is safe from cyber-attacks. . Professors have neither the time and the motivation: In my college, each class has about 60 people and teachers have to attend to multiple batches and multiple courses. It’s only natural that they try to speed up the time they spend with each student so that they can interact with all of the students . The small amount of time that students spend interacting with professors is not enough to discuss the possible ramifications of their work. A lot of times the pacing of the course is such that they are barely able to complete the syllabus let alone discuss about other issues. Even if they have some time, most of the faculty members are full time researchers. They are better off spending their time researching about their field of study rather than ensuring that students know about the ethical implications of the things they are studying. . | Build fast, don’t think about it: Hackathons and coding competitions are an important part of the college experience. Most of the hackathons take place over a period of two days (48 hours and sometimes 72 hours) where students have to implement a solution and pitch it to the judges. Now here is the problem. In these super rapid sprints of developing tech, participants do not have the time to think about the consequences and downsides of their solution. Building tech as fast as possible is a harmful approach that does not foster a sense of responsibility in the people building the tech. Even while pitching the idea , judges do not ask about how the solution can be misused or what are it’s pitfalls. . | I have a personal anecdote to share. One of my friends participated in a hackathon focused on generating solutions to ensure women safety. The solution his team came up with goes something like this: build an app that requires user location to check whether the user is in a ‘safe’ zone or a ‘dangerous’ zone. If the user is in a ‘dangerous’ zone, send them a notification warning them to keep out or inform someone of their whereabouts. At first glance, this solution seems innocuous and even helpful. But there can be some problematic uses of this application. The first obvious concern is regarding the possible leakage and misuse of user location data by companies and malintent individuals. But a more concerning issue is: Who decides which area is ‘dangerous’ and which is ‘safe’ for women. What if companies send warning notifications when the user enters areas where their competitors have set up shop. Asking people to come up with solutions to deep routed problems in our society in 48 hours simply ensures that a new and more pervasive set of problems is created. . Now that we have seen some of the reasons why students don’t think before building, we can provide some helpful suggestions: . Open-source courses are great: There is a great deal of freely available open source content about ethics in tech. I personally recommend Fast AI’s Practical Data Ethics course for anyone who wants to establish a solid understanding about ethical issues in AI. We can learn a lot about how to teach ethics to students from popular open-source courses. An important thing to notice is how practical such courses are. Teaching students about how ethics is applied in real life scenarios must be the top priority of any ethics course. Furthermore, it is important to ensure that ethics courses are based on the current issues in tech and the problems arising now. . | Ramifications section in course based projects: Pushing students to think about the ramifications of their work might be a good thing. Professors can ensure that students include a section about the ramifications of their work when they submit a project report. Although the students might not be able to nail it, it’s important they start somewhere. This may require the teachers to introduce some sort of ethical framework (this may take time) for the students to use but it does ensure that the students think about what they are building. . | Discussions, debates, and talks: We always hear about talks on the benefits of using AI or how blockchain is revolutionizing the banking sector, but we do not hear about the problems that might arise due to the widespread usage of such tech. Colleges have the responsibility of introducing to students the pitfalls and shortcomings of the current technology. Organizing discussions and talks on the problems arising due to tech seems like a reasonably good place to start from in a long journey of producing ethically sound engineers. Debates are an important way of assessing opposing ideas and arguments. They also teach students to think about counter arguments whilst they are formulating their own. . | Promoting research on ethics issues in tech: In 2020, my university had 3061 publications from researchers and students. Clearly a lot of research is being published. But there is a significant lack of research around tech ethics. If the technology institutes in India do not research about the ethical issues in technology then who will? My university has put financial incentives in place to promote students to publish papers and do research, this could be done for promoting research around tech ethics as well. People and groups respond strongly to financial incentives. Maybe if there a cash reward for publishing good research on ethics then researchers and students might focus on it. . | . Technology is a powerful enabler that brings about changes in society that lead to human progress and social good. It is the responsibility of the universities to make sure that the students that go on to become industry leaders have their minds in the right place and their hearts on the right path. .",
            "url": "https://mehulfollytobevice.github.io/My_blogs/2021/03/28/Why-don't-engineering-students-think-about-ethics.html",
            "relUrl": "/2021/03/28/Why-don't-engineering-students-think-about-ethics.html",
            "date": " • Mar 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, I’m Mehul 👋 . I’m a Student Developer and ML enthusiast! . I’m currently working on several ML and data science projects | I love creating amazing things using code | I aspire to become a ML engineer/Data scientist | Fun fact: I love to play drums | Feel free to go through my repositories , you might find something interesting | .",
          "url": "https://mehulfollytobevice.github.io/My_blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mehulfollytobevice.github.io/My_blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}